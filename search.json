[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html",
    "href": "posts/hw0/HW0_post-new.html",
    "title": "HW0 Creating Post",
    "section": "",
    "text": "In this post, we will explore the dataset of penguins that captures various aspects of their physical characteristics. We will visualize the dataset and the explore the relationship between different characteristics by using the Plotly library. We will focus on capturing the differences in Culmen Length and Culmen Depth among penguins based on their sex and the place they live. We will create some visualization to see if there is any significant difference of Culmen Length and Culmen Depth between sex or where they live."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#introduction",
    "href": "posts/hw0/HW0_post-new.html#introduction",
    "title": "HW0 Creating Post",
    "section": "",
    "text": "In this post, we will explore the dataset of penguins that captures various aspects of their physical characteristics. We will visualize the dataset and the explore the relationship between different characteristics by using the Plotly library. We will focus on capturing the differences in Culmen Length and Culmen Depth among penguins based on their sex and the place they live. We will create some visualization to see if there is any significant difference of Culmen Length and Culmen Depth between sex or where they live."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#load-the-dataset",
    "href": "posts/hw0/HW0_post-new.html#load-the-dataset",
    "title": "HW0 Creating Post",
    "section": "Load the Dataset",
    "text": "Load the Dataset\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\n\n# Load the dataset into a pandas DataFrame\npenguins = pd.read_csv(url)\n\n# First few rows of the data\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN"
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-sex",
    "href": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-sex",
    "title": "HW0 Creating Post",
    "section": "Visualizing Culmen Size by Sex",
    "text": "Visualizing Culmen Size by Sex\nFirst, we will going to see if there is a difference of Culmen Length and Depth between male and female penguins. We will plot the data in 2D plots with different scatter colors using the plotly library.\n\nimport plotly\nfrom plotly import express as px\n\nfig1 = px.scatter(data_frame = penguins,      \n                 x = \"Culmen Length (mm)\",    # column for x axis\n                 y = \"Culmen Depth (mm)\",     # column for y axis\n                 color = \"Sex\",               # column for dot color : \n                                              #   visualize the difference by sex\n                 width = 500,                 # width of figure\n                 height = 300,                # height of figure\n                 opacity = 0.5                # opacity of figure\n                )\n\n#reduce whitespace and add title\nfig1.update_layout(margin={\"r\":0, \"t\":50, \"l\":0, \"b\":0}, \n                  title = \"Culmen Length and Depth by Sex\")\nfig1.show()\n\n# Save the figure as HTML\nfrom plotly.io import write_html\nwrite_html(fig1, \"plot1.html\")\n\n                                                \n\n\nAs you can see above, overall, male penguin have deeper and longer Culmen. However, there is no clear boundary between the sex. That is because the length and the depth can be affected by other factors, especially the species of penguins."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-habitat",
    "href": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-habitat",
    "title": "HW0 Creating Post",
    "section": "Visualizing Culmen Size by Habitat",
    "text": "Visualizing Culmen Size by Habitat\nNext, we’re curious about how the habitat of a penguin might influence its Culmen size. We will again plot the 2D scatter plot using the ploty library.\n\nfig2 = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",   \n                 y = \"Culmen Depth (mm)\",    \n                 color = \"Island\",          # column for dot color : \n                                            #   visualize the difference by habitat\n                 width = 500,\n                 height = 300,\n                 opacity = 0.5\n                )\n\nfig2.update_layout(margin={\"r\":0, \"t\":50, \"l\":0, \"b\":0}, \n                  title = \"Culmen Length and Depth by Habitat\")\nfig2.show()\n\nwrite_html(fig2, \"plot2.html\")\n\n                                                \n\n\nIn the plot, we can see that overall, the penguins in Drean island have the biggest Culmen. In Torgersen, those with similar Culmen live. However, in Biscoe and Dream island penguins with various size of Culmen live together."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#conclusion",
    "href": "posts/hw0/HW0_post-new.html#conclusion",
    "title": "HW0 Creating Post",
    "section": "Conclusion",
    "text": "Conclusion\nThrough the visualiazation, we have observed the differences in Culmen size among penguins based on sex and their living islands. However, it looks like there isn’t clear and significant difference of the Culmen size by sex or habitat itself. We might need to add some more classifiers in order to detect the difference in the Culmen size."
  },
  {
    "objectID": "posts/hw3/index.html",
    "href": "posts/hw3/index.html",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "",
    "text": "URL for the Github repo : https://github.com/askpl00/flask_hw3_16B/tree/main\nIn this post, I’m going to show you how I built a simple message bank web application using Flask. This application allows users to submit messages and view a random messages that were submitted. Let’s dive into the functions that I built to make the web app."
  },
  {
    "objectID": "posts/hw3/index.html#the-main-page-route",
    "href": "posts/hw3/index.html#the-main-page-route",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "The Main Page : route(‘/’)",
    "text": "The Main Page : route(‘/’)\nTo start with, I set the main page of the website. The route (/) is typically the entry point, the first page a user sees when they visit the web app.\n@app.route('/')\ndef index():\n    return render_template('base.html')\nWhen a visitor goes to the home page, Flask executes the index function. Then it renders base.html whuch also contains the navigation and layout."
  },
  {
    "objectID": "posts/hw3/index.html#base.html",
    "href": "posts/hw3/index.html#base.html",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "base.html",
    "text": "base.html\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} A Simple Message Bank&lt;/title&gt;\n&lt;nav&gt;   # Navigation section of the webpage\n  &lt;h1&gt;A Simple Message Bank&lt;/h1&gt;  # The site's main title or heading (same with the title above)\n  &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n  &lt;ul&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a message&lt;/a&gt;&lt;/li&gt;  # Navigation link to the page for submitting messages\n    &lt;li&gt;&lt;a href=\"{{ url_for('view')}}\"&gt;View messages&lt;/a&gt;&lt;/li&gt;  # Navigation link to the page for viewing messages \n\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\nIn the base.html, I created a avigation bar to navigate to submit messages or view them. Also, I created the title, and contents, and navigation bars."
  },
  {
    "objectID": "posts/hw3/index.html#message-submission-page-routesubmit",
    "href": "posts/hw3/index.html#message-submission-page-routesubmit",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "Message Submission Page : route(‘/submit’)",
    "text": "Message Submission Page : route(‘/submit’)\nThe submit view function allows users to submit messages. It responds to both GET and POST requests. When the route receives a GET request, it renders the submit.html template, which contains a form for the user to submit their message. When a POST request is made, which happens when a user submits the form, the insert_message function is called.\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    if request.method == 'POST':\n        # If submitted, process the submitted data\n        insert_message(request)\n        return render_template('submit.html', thanks=True)\n    # If the page is requested just as GET, just show the form\n    return render_template('submit.html')\n\n![user submitting a message](Screen Shot 1.png)\n\nzsh:1: bad pattern: [user"
  },
  {
    "objectID": "posts/hw3/index.html#insert_message-function",
    "href": "posts/hw3/index.html#insert_message-function",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "insert_message function",
    "text": "insert_message function\nThe insert_message function extracts the submitted message and the user’s name and inserts them into a SQLite database.\ndef insert_message(request):\n\n    db = get_message_db()\n    cursor = db.cursor()\n    \n    # Extract the handle and message from the given data\n    handle = request.form['handle']\n    message = request.form['message']\n    \n    # Insert the new message into the messages table\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    db.commit()\n    cursor.close()  # Close the cursor\ncursor.execute() tells the database to add a new message into the messages table. It uses handle and message from the user’s input, and insert them into the database."
  },
  {
    "objectID": "posts/hw3/index.html#get_message_db-function",
    "href": "posts/hw3/index.html#get_message_db-function",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "get_message_db function",
    "text": "get_message_db function\nThis function gets a connection to the database with the messages. This function tries to return database connection from Flask’s g object. If it doesn’t exist, it makes a new connection and ensures our messages table is created.\ndef get_message_db():\n\n    try:\n        # Try to return the database connection\n        return g.message_db\n    except:\n        # If it doesn't exist, create a new database \n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        \n        # Command to create a messages table if it does not exist\n        cmd = \"\"\"\n        CREATE TABLE IF NOT EXISTS messages (\n            id INTEGER PRIMARY KEY,\n            handle TEXT,\n            message TEXT\n        )\n        \"\"\"\n        \n        # Create a cursor to execute the SQL command\n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        \n        # Return the connection\n        return g.message_db\nAfter running the code, it returns sqlite3 connection to the messages database."
  },
  {
    "objectID": "posts/hw3/index.html#message-view-page-routeview",
    "href": "posts/hw3/index.html#message-view-page-routeview",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "Message View Page : route(‘/view’)",
    "text": "Message View Page : route(‘/view’)\nView route and the random_messages function allows to show random messages\n@app.route('/view')\ndef view():\n    msgs = random_messages(5)  # 5 random messages from the database\n    return render_template('view.html', messages=msgs)"
  },
  {
    "objectID": "posts/hw3/index.html#random_messages-function",
    "href": "posts/hw3/index.html#random_messages-function",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "random_messages function",
    "text": "random_messages function\ndef random_messages(n):\n\n    db = get_message_db()\n    cursor = db.cursor()\n    \n    # Fetch n random messages from the database\n    # If there are fewer than n messages, fetch all available messages.\n    query = \"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?\"\n   \n    # Fetch the results of the query\n    cursor.execute(query, (n,))\n    random_msgs = cursor.fetchall()\n    \n    db.close()  # Close the database connection\n    return random_msgs\nThe random_messages function allows to pick specific number of random rows from the database, and return those random messages."
  },
  {
    "objectID": "posts/untitled folder/index.html",
    "href": "posts/untitled folder/index.html",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "",
    "text": "In this blog post, I will show you how to web scrape but I am going to create my own movie scraper. I will going to create TmdbSpider class which will scrape all the casts and the movies that the casts were in. To do so, I will create 3 different parse and the output will be saved in a seperate csv file."
  },
  {
    "objectID": "posts/untitled folder/index.html#setup-and-overview",
    "href": "posts/untitled folder/index.html#setup-and-overview",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Setup and Overview",
    "text": "Setup and Overview\nThe TMDB page we are going to use is https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/ which is Harry Potter and the Philosopher’s Stone.\nWe will first look through how the website looks like. Once you choose the movie, you can find the Full Cast & Crew link. Yhis will lead to page\n(original_url)cast/\nIf you scroll down, you will see the Cast section. If you click one of those, Alan Rickman, for example, then the full URL is going to be\nhttps://www.themoviedb.org/person/4566-alan-rickman\nOnce you get into the actors’s page, you will see the list of the actors’ acting, crew, production, (etc..) list."
  },
  {
    "objectID": "posts/untitled folder/index.html#write-the-scraper",
    "href": "posts/untitled folder/index.html#write-the-scraper",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Write the Scraper",
    "text": "Write the Scraper\nNow we will write a scraper using the scrapy.\nThe basic format of the scraper look like :\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nAlong here, we need to implement three parsing methods for the TmdbSpider class, which are parse(self, response), parse_full_credits(self, response), parse_actor_page(self, response).\n\nparse(self, response)\n\nInstruction :\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n    \"\"\"\n    Construct the full cast URL and navigate to the cast URL.\n    \"\"\"\n    # Make the full cast URL by adding \"/cast\" \n    full_cast_url = response.url + \"/cast\"\n    \n    # Yield a new scrapy.Request for the cast URL, and use the callback method to handle the response\n    yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n\nThe first method is pretty clear. We start from the movie main page. What this method have to do is to navigate to the Full Cast & Crew page. Once you click the page in the website, you can find out the page URL is just\n(original_url)cast/\nTherefore what we need to do here is to just add cast/ to the initial URL we have and yield a Request by spider’s callback. We can specify what should happen when we get there as a callback, which is to yielded scrapy.Request.\n\n\n\ndef parse_full_credits(self, response)\n\nInstruction :\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Select only the cast section, excluding crew, and extracts URLs to actor pages.\n    Navigates to each actor's page by using for loop.\n    \"\"\"\n    # Use selectors to extract only the cast members, which does not have class 'crew'\n    cast = response.css('ol.people.credits:not(.crew)')\n    \n    # Extract the 'href' links to the individual actor pages\n    hrefs = cast.css('div.info a::attr(href)').getall()\n\n    # Iterate for each actor page\n    for link in hrefs:\n        yield scrapy.Request(response.urljoin(link), callback=self.parse_actor_page)\n\nNow this method starts from the Full Cast & Crew page. We first need to request the page or each actor(only actor, not crew). Here we are going to use the selector. Each actor’s selector is in this format:\n&lt;div class=\"info\"&gt;\n            &lt;p&gt;&lt;a href=\"/person/194-richard-harris\"&gt;Richard Harris&lt;/a&gt;&lt;/p&gt;&lt;p&gt;\n            &lt;/p&gt;&lt;p class=\"character\"&gt;Albus Dumbledore\n            &lt;/p&gt;\n          &lt;/div&gt;\nHowever, not only actor has this format, but also crew has this format. So we have to look for higher class. Then we can find that the casting is under\n&lt;ol class=\"people credits \"&gt;\nand the crew is under\n&lt;ol class=\"people credits crew\"&gt;\nSo I first set the\ncast = response.css('ol.people.credits:not(.crew)') \nand then set\nhrefs = cast.css('div.info a::attr(href)').getall().\nsince what we see is in a hyperlink format, I wrote as a::attr(href) under div class info. I used getall() because we need to navigate to all the cast.\nThe for loop is to navigate each of the cast’s page and here request and callback self.parse_actor_page.\n\n\n\ndef parse_actor_page(self, response):\n\nInstruction :\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {“actor” : actor_name, “movie_or_TV_name” : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an “Acting” role1. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Extract the actor's name on the actor's page.\n    Extract movie's name by selecting the table that follows after the \"Acting\".\n    Create a unique set to avoid duplicates and yields actor's name and movie's name.\n    \"\"\"\n    # Extract actor's name from h2 tag class title\n    actor_name = response.css('h2.title ::text').get()\n\n    # Extract the table that follows the h3\n    acting_section = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[1]\")\n\n    # Create a set to store movie titles.\n    unique_movies = set()\n\n    for movie in acting_section.css(\"tr\"):\n        # Extract the text, which is the movie name\n        movie_name = movie.css('td a ::text').get()  \n\n        # Add the movie name to the set if it is not already in the set\n        if movie_name not in unique_movies:\n            unique_movies.add(movie_name)  \n\n            yield {\n                'actor': actor_name,\n                'movie_or_TV_name': movie_name\n            }\n\nWe start from actor’s page. First we will going to extract the actor’s name by calling\nactor_name = response.css('h2.title ::text').get()\nThe name is in h2.title format. So we extract just the text by ::text.\nNext we need to find what movie they acted in. We should only yield movies or TV shows on which that actor has worked in an “Acting” role. However, “Acting”, “crew”, “Production” are all under same  format:\n&lt;tbody&gt;&lt;tr&gt;\n          &lt;td&gt;\n            &lt;table class=\"credit_group\"&gt;\n              &lt;tbody&gt;&lt;tr&gt;\n                &lt;td class=\"year\"&gt;—&lt;/td&gt;\n                  &lt;td class=\"seperator\"&gt;&lt;span data-url=\"/tv/44337\" data-id=\"52596773760ee346619c97e4\" data-type=\"tv\" data-slug=\"44337\" class=\"glyphicons_v2 circle-empty account_adult_false item_adult_false\"&gt;&lt;/span&gt;&lt;/td&gt;\n                &lt;td class=\"role true account_adult_false item_adult_false\"&gt;\n                  &lt;a class=\"tooltip\" href=\"/tv/44337\"&gt;&lt;bdi&gt;Have I Got a Bit More News for You&lt;/bdi&gt;&lt;/a&gt;\n                    &lt;span class=\"group\"&gt; &lt;span&gt;(&lt;a class=\"tv\" href=\"/tv/44337/episodes?credit_id=5bce0b02c3a3683d6f000fd7&amp;person_id=4bc89155017a3c122d00c255\"&gt;2 episodes&lt;/a&gt;)&lt;/span&gt; as &lt;span class=\"character\"&gt;Self - Presenter&lt;/span&gt;&lt;/span&gt;\n                &lt;/td&gt;\n              &lt;/tr&gt;\n            &lt;/tbody&gt;&lt;/table&gt;\n          &lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/tbody&gt;\n    \nThe only thing that is different is each acting, crew or production are under the different h3 class zero title.\n&lt;h3 class=\"zero\"&gt;Acting&lt;/h3&gt;\n&lt;h3 class=\"zero\"&gt;Crew&lt;/h3&gt;\n&lt;h3 class=\"zero\"&gt;Production&lt;/h3&gt;\nHowever, the tables are not under these selectors, but the tables follow right after the titles. So we can extract the ‘Acting’ section by selecting the table right after this class.\nacting_section = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[1]\")\nThe code above is first filter the previously selected h3 elements. It selects only those content that contains the word “Acting”. And then find siblings that come after the selected node. Then select the first table element, table[1], which is a following sibling of the h3 containing the word “Acting”.\nIn this way we can get the movie name of only “Acting”."
  },
  {
    "objectID": "posts/untitled folder/index.html#run-the-scraper",
    "href": "posts/untitled folder/index.html#run-the-scraper",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Run the Scraper",
    "text": "Run the Scraper\nNow we can run the following command in the terminal inside the directory you want.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nThis command will make a csv file named results in the folder you want."
  },
  {
    "objectID": "posts/untitled folder/index.html#make-the-recommendations",
    "href": "posts/untitled folder/index.html#make-the-recommendations",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Make the Recommendations",
    "text": "Make the Recommendations\nBased on the data of actors and the movie they acted in, we can make a recommandation of the specific movie(here it is Harry Potter and the Philosopher’s Stone).\n\nimport pandas as pd\n\n# Load the CSV file\nresults_df = pd.read_csv('/Users/gimdong-gyu/Desktop/TMDB_scraper/results.csv')\n\nresults_df\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nDaniel Radcliffe\nMulligan\n\n\n4\nDaniel Radcliffe\nDigman!\n\n\n...\n...\n...\n\n\n2917\nRupert Grint\nThe View\n\n\n2918\nRupert Grint\nGMTV\n\n\n2919\nRupert Grint\nThe Tonight Show with Jay Leno\n\n\n2920\nRupert Grint\nAn Audience with...\n\n\n2921\nRupert Grint\nToday\n\n\n\n\n2922 rows × 2 columns\n\n\n\nWe can see there are 2922 rows, which means there are total 2922 movies that every cast had a roll in.\nBut we need to group those actors that were in the same movie and count them in order to see which movie has the most shared actor. Also we can make a recommandation based on the most shared movies.\n\n# group by the 'movie_or_TV_name' and count the number of unique actors.\nshared_actors = results_df.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\nshared_actors.columns = ['movie names', 'number of shared actors']\n\n# Sort by the number of shared actors \nshared_sorted = shared_actors.sort_values(by='number of shared actors', ascending=False)\nshared_sorted\n\n\n\n\n\n\n\n\nmovie names\nnumber of shared actors\n\n\n\n\n700\nHarry Potter and the Philosopher's Stone\n63\n\n\n694\nHarry Potter and the Chamber of Secrets\n37\n\n\n382\nCreating the World of Harry Potter\n36\n\n\n701\nHarry Potter and the Prisoner of Azkaban\n26\n\n\n699\nHarry Potter and the Order of the Phoenix\n24\n\n\n...\n...\n...\n\n\n796\nIndiana Jones and the Kingdom of the Crystal S...\n1\n\n\n795\nIndian Summers\n1\n\n\n794\nIn the Red\n1\n\n\n793\nIn the Heart of the Sea\n1\n\n\n2273\nZastrozzi: A Romance\n1\n\n\n\n\n2274 rows × 2 columns\n\n\n\nNow we want to visualize the top 10 movies that shows the most shared actors. To do this, We can use matplotlib to create a bar chart\n\nimport matplotlib.pyplot as plt\n\n# Get the top 10 movies/TV shows\ntop10 = shared_sorted.head(10)\n\n# Plotting the bar chart\nplt.figure(figsize=(10, 6))\nplt.barh(top10['movie names'], top10['number of shared actors'])\nplt.title('Top 10 Movies with the Most Shared Actors')\nplt.xlabel('Shared Actors Count')\nplt.ylabel('Movie')\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph, we can see that the Harry Potter and the Chamber of Secrets is the movie that has most shared actors with Harry Potter and the Philosopher’s Stone.\nHowever, this recommandation is not really useful because except for one movie, those movies in top 10 are all Harry Potter Series.\nTherefore We will make a seperate recommandation plot that does not contain Harry Potter Series.\n\nRecommandation Other than Harry Potter Series\n\n# Remove movies that contain \"Harry Potter\" in the title.\nno_harry = shared_sorted[~shared_sorted['movie names'].str.contains('Harry Potter')]\n\ntop10 = no_harry.head(10)\n\nplt.figure(figsize=(10, 6))\nplt.barh(top10['movie names'], top10['number of shared actors'])\nplt.title('Top 10 Movies with the Most Shared Actors (Other than Harry Potter)')\nplt.xlabel('Shared Actors Count')\nplt.ylabel('Movie')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plot, now we can see that the most most shared actor movie is Doctor Who and all the movies in the top 10 are now movies other than Harry Potter."
  },
  {
    "objectID": "posts/untitled folder/index.html#conclusion",
    "href": "posts/untitled folder/index.html#conclusion",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we looked through how we can apply scrapy to make a recommandation system. Web Scraping is a powerful tool to gather various data from the web."
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "HW1 : Visualizing Climate Data using Query & Database",
    "section": "",
    "text": "In this blog post, we’ll look through interesting dataset on global temperatures and explore various ways of visualize using Python, SQLite, and Plotly. We will create interactive visualizations that can help us understand worldwide temperature over time.\n\n# To properly show figures in your blog,\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/hw1/index.html#yearly-temperature-variation",
    "href": "posts/hw1/index.html#yearly-temperature-variation",
    "title": "HW1 : Visualizing Climate Data using Query & Database",
    "section": "1) Yearly Temperature Variation",
    "text": "1) Yearly Temperature Variation\nQuestion: How has the temperature variation (difference between the highest and lowest temperatures) changed over the years for a given country?\nTo answer this question, yearly_temperature_variation will calculate the yearly temperature variation for a specified country and range of years.\nThis is what the yearly_temperature_variation query function looks like :\n\nfrom climate_database import yearly_temperature_variation\nimport inspect\nprint(inspect.getsource(yearly_temperature_variation))\n\ndef yearly_temperature_variation(db_file, country, start_year, end_year):\n    \"\"\"\n    Yearly temperature variation (difference between max and min temperatures) \n    for a given country and start year and end year.\n    \n    Returns: Dataframe with two columns: Year and Variation,\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    query = f\"\"\"\n    SELECT T.Year, (MAX(T.Temp) - MIN(T.Temp)) AS Variation\n    FROM temperatures T\n    JOIN stations S ON T.ID = S.ID\n    JOIN countries C ON substr(T.ID, 1, 2) = C.[FIPS 10-4]\n    WHERE C.Name = ? AND T.Year BETWEEN ? AND ?\n    GROUP BY T.Year\n    ORDER BY T.Year;\n    \"\"\"\n    df = pd.read_sql_query(query, conn, params=(country, start_year, end_year))\n    conn.close()\n    return df\n\n\n\nWith the plot_yearly_temperature_variation, we can create a line plot showing the temperature variation of a specific country.\n\ndef plot_yearly_temperature_variation(db_file, country, start_year, end_year):\n    \"\"\"\n    Creates a line plot showing the yearly temperature variation.\n    \"\"\"\n    \n    df = yearly_temperature_variation(db_file, country, start_year, end_year)\n    fig = px.line(df, x='Year', y='Variation', \n                  title=f'Yearly Temperature Variation in {country}')\n    fig.update_xaxes(title_text='Year')\n    fig.update_yaxes(title_text='Temperature Variation (°C)')\n    return fig\n\n\nplot_yearly_temperature_variation(\"climate_data.db\", \"Japan\", 1960, 2000)"
  },
  {
    "objectID": "posts/hw1/index.html#extreme-temperature-on-specific-year-by-month",
    "href": "posts/hw1/index.html#extreme-temperature-on-specific-year-by-month",
    "title": "HW1 : Visualizing Climate Data using Query & Database",
    "section": "2) Extreme Temperature on Specific year by Month",
    "text": "2) Extreme Temperature on Specific year by Month\nQuestion : “What are the hottest and coldest temperatures for each month in a given year?”\nThe second query function is plot_extreme_temperatures(df, year) function which get the extreme temperature, the highest and the lowest, in the world when you put the input year. It will show you facet plot with the lowest and the highest temperatre plots in seperate figure.\n\nfrom climate_database import extreme_temperatures\nimport inspect\nprint(inspect.getsource(extreme_temperatures))\n\ndef extreme_temperatures(db_file, year):\n    \"\"\"\n    Lowest and highest temperatures for each month in specific year.\n    \n    Returns: Dataframe containing the lowest and highest temperatures for each month \n    in the world.\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cmd = f\"\"\"\n    SELECT {year} AS Year, Month,\n           MIN(Temp) AS Min_Temperature,\n           MAX(Temp) AS Max_Temperature\n    FROM temperatures\n    WHERE Year = '{year}'\n    GROUP BY Month\n    ORDER BY Month\n    \"\"\"\n    df = pd.read_sql_query(cmd, conn)\n    # Prepare the DataFrame for faceting by melting it\n    df_melted = df.melt(id_vars=['Month'], value_vars=['Min_Temperature', 'Max_Temperature'],\n                        var_name='Temperature_Type', value_name='Temperature')\n    conn.close()\n    return df_melted\n\n\n\n\nimport plotly.express as px\n\ndef plot_extreme_temperatures(df, year):\n    \"\"\"\n    Creates a faceted line plot showing the lowest and highest temperatures for each month.\n    \n    Parameters:\n    - df (DataFrame): DataFrame containing the lowest and highest temperatures for each month, \n        which we got from the query.\n    - year (int): The year for which extreme temperatures are visualized.\n    \"\"\"\n    # Use the DataFrame directly for plotting with Plotly Express\n    fig = px.line(df, x='Month', y='Temperature', color='Temperature_Type', \n                  facet_col='Temperature_Type', title=f'Extreme Temperatures in {year}')\n    \n    # Update layout for better readability\n    fig.update_layout(xaxis_title='Month', yaxis_title='Temperature (°C)')\n    fig.show()\n\n\n# Plot using the year you want \n# Ex. year 2000\nplot_extreme_temperatures(extreme_temperatures(\"climate_data.db\", 2000), 2000)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BlogPic16B",
    "section": "",
    "text": "HW3 : Message Bank Web Development Using Flask\n\n\n\n\n\n\nweek 4\n\n\nweek 5\n\n\nHW\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW2 : Scrapying TMDB Website & Movie Recommandation\n\n\n\n\n\n\nweek 3\n\n\nweek 4\n\n\nHW\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW1 : Visualizing Climate Data using Query & Database\n\n\n\n\n\n\nweek 2\n\n\nweek 3\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW0 Creating Post\n\n\n\n\n\n\nweek 1\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\nNo matching items"
  }
]