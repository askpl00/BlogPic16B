[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html",
    "href": "posts/hw0/HW0_post-new.html",
    "title": "HW0 Creating Post",
    "section": "",
    "text": "In this post, we will explore the dataset of penguins that captures various aspects of their physical characteristics. We will visualize the dataset and the explore the relationship between different characteristics by using the Plotly library. We will focus on capturing the differences in Culmen Length and Culmen Depth among penguins based on their sex and the place they live. We will create some visualization to see if there is any significant difference of Culmen Length and Culmen Depth between sex or where they live."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#introduction",
    "href": "posts/hw0/HW0_post-new.html#introduction",
    "title": "HW0 Creating Post",
    "section": "",
    "text": "In this post, we will explore the dataset of penguins that captures various aspects of their physical characteristics. We will visualize the dataset and the explore the relationship between different characteristics by using the Plotly library. We will focus on capturing the differences in Culmen Length and Culmen Depth among penguins based on their sex and the place they live. We will create some visualization to see if there is any significant difference of Culmen Length and Culmen Depth between sex or where they live."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#load-the-dataset",
    "href": "posts/hw0/HW0_post-new.html#load-the-dataset",
    "title": "HW0 Creating Post",
    "section": "Load the Dataset",
    "text": "Load the Dataset\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\n\n# Load the dataset into a pandas DataFrame\npenguins = pd.read_csv(url)\n\n# First few rows of the data\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN"
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-sex",
    "href": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-sex",
    "title": "HW0 Creating Post",
    "section": "Visualizing Culmen Size by Sex",
    "text": "Visualizing Culmen Size by Sex\nFirst, we will going to see if there is a difference of Culmen Length and Depth between male and female penguins. We will plot the data in 2D plots with different scatter colors using the plotly library.\n\nimport plotly\nfrom plotly import express as px\n\nfig1 = px.scatter(data_frame = penguins,      \n                 x = \"Culmen Length (mm)\",    # column for x axis\n                 y = \"Culmen Depth (mm)\",     # column for y axis\n                 color = \"Sex\",               # column for dot color : \n                                              #   visualize the difference by sex\n                 width = 500,                 # width of figure\n                 height = 300,                # height of figure\n                 opacity = 0.5                # opacity of figure\n                )\n\n#reduce whitespace and add title\nfig1.update_layout(margin={\"r\":0, \"t\":50, \"l\":0, \"b\":0}, \n                  title = \"Culmen Length and Depth by Sex\")\nfig1.show()\n\n# Save the figure as HTML\nfrom plotly.io import write_html\nwrite_html(fig1, \"plot1.html\")\n\n                                                \n\n\nAs you can see above, overall, male penguin have deeper and longer Culmen. However, there is no clear boundary between the sex. That is because the length and the depth can be affected by other factors, especially the species of penguins."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-habitat",
    "href": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-habitat",
    "title": "HW0 Creating Post",
    "section": "Visualizing Culmen Size by Habitat",
    "text": "Visualizing Culmen Size by Habitat\nNext, we’re curious about how the habitat of a penguin might influence its Culmen size. We will again plot the 2D scatter plot using the ploty library.\n\nfig2 = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",   \n                 y = \"Culmen Depth (mm)\",    \n                 color = \"Island\",          # column for dot color : \n                                            #   visualize the difference by habitat\n                 width = 500,\n                 height = 300,\n                 opacity = 0.5\n                )\n\nfig2.update_layout(margin={\"r\":0, \"t\":50, \"l\":0, \"b\":0}, \n                  title = \"Culmen Length and Depth by Habitat\")\nfig2.show()\n\nwrite_html(fig2, \"plot2.html\")\n\n                                                \n\n\nIn the plot, we can see that overall, the penguins in Drean island have the biggest Culmen. In Torgersen, those with similar Culmen live. However, in Biscoe and Dream island penguins with various size of Culmen live together."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#conclusion",
    "href": "posts/hw0/HW0_post-new.html#conclusion",
    "title": "HW0 Creating Post",
    "section": "Conclusion",
    "text": "Conclusion\nThrough the visualiazation, we have observed the differences in Culmen size among penguins based on sex and their living islands. However, it looks like there isn’t clear and significant difference of the Culmen size by sex or habitat itself. We might need to add some more classifiers in order to detect the difference in the Culmen size."
  },
  {
    "objectID": "posts/hw6/index_hw6.html",
    "href": "posts/hw6/index_hw6.html",
    "title": "HW5 : Text Classification : Spotting Fake News",
    "section": "",
    "text": "Introduction\nIn this blog post, I will show you how to detect fake news using Keras text classification.\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\nimport keras\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\nData Preparation\nWe first need to load the dataset containing news articles labeled as real or fake.\n\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf = pd.read_csv(train_url)\n\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow we are going to define the function called make_dataset. this function will return a dataset and clean the data; remove stopwords from the article text and title, using the nltk. We will change the text to lowercase and remove stopwords to ensure our models do not detect unnecessary words.\n\n\ndef make_dataset(df):\n    # Change into lowercase\n    df['title'] = df['title'].str.lower()\n    df['text'] = df['text'].str.lower()\n\n    # Import stopwords with nltk.\n    import nltk\n    nltk.download('stopwords')\n    from nltk.corpus import stopwords\n    stop = stopwords.words('english')\n\n    # Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n    # Simailr to what was on the StackOverFlow thread\n    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n    data = tf.data.Dataset.from_tensor_slices(\n        (\n            {\n                \"title\": df['title'],\n                \"text\": df['text']\n            },\n            df['fake'].values\n        )\n    )\n\n    # Shuffle and batch the dataset\n    data = data.shuffle(buffer_size=len(df), reshuffle_each_iteration=False)\n    data = data.batch(100)\n\n    return data\n\n\ndata = make_dataset(df)\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nNow that we’ve constructed our primary Dataset, we will split 80% for training and 20% for validation.\n\ntrain_size = int(0.8 * len(data))\nval_size = int(0.2 * len(data))\n\n# Split the dataset\ntrain_dataset = data.take(train_size)\nval_dataset = data.skip(train_size).take(val_size)\n\nlen(train_dataset), len(val_dataset)\n\n(180, 45)\n\n\n\n\nBase Rate\nBase rate refers to the accuracy of a model that always makes the same guess. Now we will determine the base rate for this data set by examining the labels on the training set.\n\nbase_rate = max(df['fake'].mean(), 1 - df['fake'].mean())\n\nprint(f'Base rate: {base_rate}')\n\nBase rate: 0.522963160942581\n\n\n\n\nVectorization\nBefore we build our model to detect fake news, we need to transform text data so that our machine learning models can understand and process, and this is called vectorization. We have to vectorize both text and the title. Vectorization will convert those text into numerical vectors.\n\n# preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\n# Define standardization function\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                              '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation\n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,  # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train_dataset.map(lambda x, y: x[\"title\"]))\n\n\n\n# Same for the 'text' column\ntext_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,\n    output_mode='int',\n    output_sequence_length=500)\n\ntext_vectorize_layer.adapt(train_dataset.map(lambda x, y: x[\"text\"]))\n\n\n\nModel 1: Title-Based Detection\nThe first model will only use the titles of articles to predict whether the news is fake or real.\n\n# Input\ntitle_input = keras.Input(shape=(1,),\n                          name='title',\n                          dtype='string')\n\n# layers for processing the titles, similar to 'lyrics_features' from the lecture notes\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = layers.Embedding(size_vocabulary, 3, name=\"title_embedding\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(32, activation=\"relu\")(title_features)\n\n# Output layer for binary classification\noutput = layers.Dense(1, activation=\"sigmoid\", name=\"fake\")(title_features)\n\n\nmodel1 = keras.Model(inputs=title_input, \n                     outputs=output)\n\nmodel1.compile(optimizer=\"adam\",\n                         loss=losses.BinaryCrossentropy(from_logits=False),\n                         metrics=[\"accuracy\"])\n\nmodel1.summary()\n\n# Train the model\nhistory1 = model1.fit(train_dataset, validation_data=val_dataset, epochs=20)\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n title (InputLayer)          [(None, 1)]               0         \n                                                                 \n text_vectorization (TextVe  (None, 500)               0         \n ctorization)                                                    \n                                                                 \n title_embedding (Embedding  (None, 500, 3)            6000      \n )                                                               \n                                                                 \n dropout (Dropout)           (None, 500, 3)            0         \n                                                                 \n global_average_pooling1d (  (None, 3)                 0         \n GlobalAveragePooling1D)                                         \n                                                                 \n dropout_1 (Dropout)         (None, 3)                 0         \n                                                                 \n dense (Dense)               (None, 32)                128       \n                                                                 \n fake (Dense)                (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 6161 (24.07 KB)\nTrainable params: 6161 (24.07 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/20\n180/180 [==============================] - 4s 13ms/step - loss: 0.6918 - accuracy: 0.5241 - val_loss: 0.6923 - val_accuracy: 0.5147\nEpoch 2/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.6895 - accuracy: 0.5258 - val_loss: 0.6872 - val_accuracy: 0.5147\nEpoch 3/20\n180/180 [==============================] - 2s 13ms/step - loss: 0.6761 - accuracy: 0.5854 - val_loss: 0.6589 - val_accuracy: 0.6316\nEpoch 4/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.6254 - accuracy: 0.7467 - val_loss: 0.5790 - val_accuracy: 0.8478\nEpoch 5/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.5350 - accuracy: 0.8308 - val_loss: 0.4772 - val_accuracy: 0.8685\nEpoch 6/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.4489 - accuracy: 0.8527 - val_loss: 0.3959 - val_accuracy: 0.8829\nEpoch 7/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.3832 - accuracy: 0.8696 - val_loss: 0.3382 - val_accuracy: 0.8921\nEpoch 8/20\n180/180 [==============================] - 3s 14ms/step - loss: 0.3364 - accuracy: 0.8829 - val_loss: 0.2963 - val_accuracy: 0.9000\nEpoch 9/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.3042 - accuracy: 0.8894 - val_loss: 0.2657 - val_accuracy: 0.9103\nEpoch 10/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2763 - accuracy: 0.8988 - val_loss: 0.2394 - val_accuracy: 0.9195\nEpoch 11/20\n180/180 [==============================] - 3s 18ms/step - loss: 0.2534 - accuracy: 0.9074 - val_loss: 0.2200 - val_accuracy: 0.9247\nEpoch 12/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2352 - accuracy: 0.9156 - val_loss: 0.2065 - val_accuracy: 0.9276\nEpoch 13/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2246 - accuracy: 0.9177 - val_loss: 0.1932 - val_accuracy: 0.9303\nEpoch 14/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2111 - accuracy: 0.9214 - val_loss: 0.1829 - val_accuracy: 0.9341\nEpoch 15/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.1995 - accuracy: 0.9221 - val_loss: 0.1760 - val_accuracy: 0.9371\nEpoch 16/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.1946 - accuracy: 0.9273 - val_loss: 0.1676 - val_accuracy: 0.9391\nEpoch 17/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1854 - accuracy: 0.9303 - val_loss: 0.1621 - val_accuracy: 0.9393\nEpoch 18/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1818 - accuracy: 0.9307 - val_loss: 0.1578 - val_accuracy: 0.9425\nEpoch 19/20\n180/180 [==============================] - 3s 14ms/step - loss: 0.1754 - accuracy: 0.9318 - val_loss: 0.1539 - val_accuracy: 0.9429\nEpoch 20/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1708 - accuracy: 0.9344 - val_loss: 0.1502 - val_accuracy: 0.9416\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(history1.history[\"accuracy\"],label='training')\nplt.plot(history1.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nThe training results show the model’s success in distinguishing real and fake news reaching about 94% accuracy on unseen data.\n\nfrom keras import utils\nutils.plot_model(model1, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nModel 2 : Text-Based Detection\nOur second model focus to the full text of the articles for the detection.\n\ntext_input = keras.Input(shape=(1,),\n                         name='text',\n                         dtype='string')\n\n# Similar with what we did for the title features\ntext_features = text_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, 3, name=\"text_embedding\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(32, activation=\"relu\")(text_features)\n\n# Output layer for binary classification\noutput = layers.Dense(1, activation=\"sigmoid\", name=\"fake\")(text_features)\n\nmodel2 = keras.Model(inputs=text_input, \n                     outputs=output)\n\nmodel2.compile(optimizer=\"adam\",\n               loss=losses.BinaryCrossentropy(from_logits=False),\n               metrics=[\"accuracy\"])\n\nmodel2.summary()\n\n# Train the model with the full text \nhistory2 = model2.fit(train_dataset,\n                      validation_data=val_dataset,\n                      epochs=20)\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text (InputLayer)           [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (Text  (None, 500)               0         \n Vectorization)                                                  \n                                                                 \n text_embedding (Embedding)  (None, 500, 3)            6000      \n                                                                 \n dropout_2 (Dropout)         (None, 500, 3)            0         \n                                                                 \n global_average_pooling1d_1  (None, 3)                 0         \n  (GlobalAveragePooling1D)                                       \n                                                                 \n dropout_3 (Dropout)         (None, 3)                 0         \n                                                                 \n dense_1 (Dense)             (None, 32)                128       \n                                                                 \n fake (Dense)                (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 6161 (24.07 KB)\nTrainable params: 6161 (24.07 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/20\n180/180 [==============================] - 5s 21ms/step - loss: 0.6782 - accuracy: 0.5561 - val_loss: 0.6399 - val_accuracy: 0.6775\nEpoch 2/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.5333 - accuracy: 0.8536 - val_loss: 0.4088 - val_accuracy: 0.9371\nEpoch 3/20\n180/180 [==============================] - 4s 19ms/step - loss: 0.3365 - accuracy: 0.9269 - val_loss: 0.2592 - val_accuracy: 0.9519\nEpoch 4/20\n180/180 [==============================] - 5s 25ms/step - loss: 0.2403 - accuracy: 0.9456 - val_loss: 0.1941 - val_accuracy: 0.9618\nEpoch 5/20\n180/180 [==============================] - 4s 21ms/step - loss: 0.1937 - accuracy: 0.9545 - val_loss: 0.1589 - val_accuracy: 0.9661\nEpoch 6/20\n180/180 [==============================] - 4s 19ms/step - loss: 0.1652 - accuracy: 0.9603 - val_loss: 0.1368 - val_accuracy: 0.9699\nEpoch 7/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.1442 - accuracy: 0.9662 - val_loss: 0.1214 - val_accuracy: 0.9726\nEpoch 8/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.1316 - accuracy: 0.9676 - val_loss: 0.1113 - val_accuracy: 0.9735\nEpoch 9/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.1188 - accuracy: 0.9698 - val_loss: 0.1024 - val_accuracy: 0.9766\nEpoch 10/20\n180/180 [==============================] - 5s 28ms/step - loss: 0.1086 - accuracy: 0.9734 - val_loss: 0.0953 - val_accuracy: 0.9768\nEpoch 11/20\n180/180 [==============================] - 4s 23ms/step - loss: 0.1011 - accuracy: 0.9725 - val_loss: 0.0905 - val_accuracy: 0.9777\nEpoch 12/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.0952 - accuracy: 0.9759 - val_loss: 0.0854 - val_accuracy: 0.9784\nEpoch 13/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0899 - accuracy: 0.9764 - val_loss: 0.0815 - val_accuracy: 0.9802\nEpoch 14/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.0843 - accuracy: 0.9777 - val_loss: 0.0786 - val_accuracy: 0.9802\nEpoch 15/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.0791 - accuracy: 0.9789 - val_loss: 0.0759 - val_accuracy: 0.9802\nEpoch 16/20\n180/180 [==============================] - 4s 23ms/step - loss: 0.0758 - accuracy: 0.9799 - val_loss: 0.0736 - val_accuracy: 0.9804\nEpoch 17/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.0733 - accuracy: 0.9800 - val_loss: 0.0720 - val_accuracy: 0.9804\nEpoch 18/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.0685 - accuracy: 0.9809 - val_loss: 0.0697 - val_accuracy: 0.9818\nEpoch 19/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0637 - accuracy: 0.9817 - val_loss: 0.0686 - val_accuracy: 0.9813\nEpoch 20/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.0631 - accuracy: 0.9827 - val_loss: 0.0671 - val_accuracy: 0.9813\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(history2.history[\"accuracy\"],label='training')\nplt.plot(history2.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nThe training results for model 2, focusing on article text, shows better performance with the accuracy of approximately 98%. This indicates that full text provides richer index for distinguishing authenticity than the title.\n\nfrom keras import utils\nutils.plot_model(model2, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nModel 3: Combination of Title and Text Detection\nThe third model is the combination of both titles and texts to make predictions. We could assume that the combination of title and article content provides the best indicator.\n\n# Concatenate title and text features\ncombined_features = layers.concatenate([title_features, text_features])\n\n# Dense layers for classification\ncombined_features = layers.Dense(64, activation=\"relu\")(combined_features)\ncombined_features = layers.Dropout(0.5)(combined_features)\noutput = layers.Dense(1, activation=\"sigmoid\", name=\"fake\")(combined_features)\n\nmodel3 = keras.Model(inputs=[title_input, text_input], \n                     outputs=output)\n\nmodel3.compile(optimizer=\"adam\",\n               loss=losses.BinaryCrossentropy(from_logits=False),\n               metrics=[\"accuracy\"])\n\nmodel3.summary()\n\n# Training the model with both title and text\nhistory3 = model3.fit(train_dataset,\n                      validation_data=val_dataset,\n                      epochs=20)\n\nModel: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n title (InputLayer)          [(None, 1)]                  0         []                            \n                                                                                                  \n text (InputLayer)           [(None, 1)]                  0         []                            \n                                                                                                  \n text_vectorization (TextVe  (None, 500)                  0         ['title[0][0]']               \n ctorization)                                                                                     \n                                                                                                  \n text_vectorization_1 (Text  (None, 500)                  0         ['text[0][0]']                \n Vectorization)                                                                                   \n                                                                                                  \n title_embedding (Embedding  (None, 500, 3)               6000      ['text_vectorization[0][0]']  \n )                                                                                                \n                                                                                                  \n text_embedding (Embedding)  (None, 500, 3)               6000      ['text_vectorization_1[0][0]']\n                                                                                                  \n dropout (Dropout)           (None, 500, 3)               0         ['title_embedding[0][0]']     \n                                                                                                  \n dropout_2 (Dropout)         (None, 500, 3)               0         ['text_embedding[0][0]']      \n                                                                                                  \n global_average_pooling1d (  (None, 3)                    0         ['dropout[0][0]']             \n GlobalAveragePooling1D)                                                                          \n                                                                                                  \n global_average_pooling1d_1  (None, 3)                    0         ['dropout_2[0][0]']           \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n dropout_1 (Dropout)         (None, 3)                    0         ['global_average_pooling1d[0][\n                                                                    0]']                          \n                                                                                                  \n dropout_3 (Dropout)         (None, 3)                    0         ['global_average_pooling1d_1[0\n                                                                    ][0]']                        \n                                                                                                  \n dense (Dense)               (None, 32)                   128       ['dropout_1[0][0]']           \n                                                                                                  \n dense_1 (Dense)             (None, 32)                   128       ['dropout_3[0][0]']           \n                                                                                                  \n concatenate (Concatenate)   (None, 64)                   0         ['dense[0][0]',               \n                                                                     'dense_1[0][0]']             \n                                                                                                  \n dense_2 (Dense)             (None, 64)                   4160      ['concatenate[0][0]']         \n                                                                                                  \n dropout_4 (Dropout)         (None, 64)                   0         ['dense_2[0][0]']             \n                                                                                                  \n fake (Dense)                (None, 1)                    65        ['dropout_4[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 16481 (64.38 KB)\nTrainable params: 16481 (64.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\nEpoch 1/20\n180/180 [==============================] - 8s 33ms/step - loss: 0.1938 - accuracy: 0.9414 - val_loss: 0.0499 - val_accuracy: 0.9888\nEpoch 2/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0436 - accuracy: 0.9896 - val_loss: 0.0382 - val_accuracy: 0.9892\nEpoch 3/20\n180/180 [==============================] - 6s 31ms/step - loss: 0.0338 - accuracy: 0.9911 - val_loss: 0.0353 - val_accuracy: 0.9897\nEpoch 4/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0297 - accuracy: 0.9913 - val_loss: 0.0332 - val_accuracy: 0.9901\nEpoch 5/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0250 - accuracy: 0.9934 - val_loss: 0.0338 - val_accuracy: 0.9901\nEpoch 6/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0248 - accuracy: 0.9927 - val_loss: 0.0349 - val_accuracy: 0.9894\nEpoch 7/20\n180/180 [==============================] - 6s 34ms/step - loss: 0.0224 - accuracy: 0.9929 - val_loss: 0.0355 - val_accuracy: 0.9903\nEpoch 8/20\n180/180 [==============================] - 5s 25ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.0323 - val_accuracy: 0.9910\nEpoch 9/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0198 - accuracy: 0.9938 - val_loss: 0.0306 - val_accuracy: 0.9912\nEpoch 10/20\n180/180 [==============================] - 11s 60ms/step - loss: 0.0173 - accuracy: 0.9949 - val_loss: 0.0305 - val_accuracy: 0.9912\nEpoch 11/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.0292 - val_accuracy: 0.9910\nEpoch 12/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0161 - accuracy: 0.9953 - val_loss: 0.0294 - val_accuracy: 0.9917\nEpoch 13/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0151 - accuracy: 0.9951 - val_loss: 0.0297 - val_accuracy: 0.9910\nEpoch 14/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.0320 - val_accuracy: 0.9908\nEpoch 15/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0133 - accuracy: 0.9963 - val_loss: 0.0299 - val_accuracy: 0.9924\nEpoch 16/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0110 - accuracy: 0.9968 - val_loss: 0.0306 - val_accuracy: 0.9924\nEpoch 17/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.0314 - val_accuracy: 0.9917\nEpoch 18/20\n180/180 [==============================] - 5s 25ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.0296 - val_accuracy: 0.9919\nEpoch 19/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0336 - val_accuracy: 0.9908\nEpoch 20/20\n180/180 [==============================] - 6s 35ms/step - loss: 0.0130 - accuracy: 0.9963 - val_loss: 0.0305 - val_accuracy: 0.9915\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(history3.history[\"accuracy\"],label='training')\nplt.plot(history3.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nModel 3 shows very high validation accuracy, achieving over 99% accuracy on validation data. We can think that by considering both titles and article texts, we can detect almost every fake or real news with the model.\n\nfrom keras import utils\nutils.plot_model(model3, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nAfter comparing the performance of each model on validation data, using both the title and text of articles for fake news detection is most effective. Model 3 validation accuracy of 99% proves that combining these elements could achive more accurate assessment.\n\n\nFinal Model Evaluation\nNow we’ll test the best model performance on unseen test data.\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\ntest_df.head()\n\ntest_dataset = make_dataset(test_df)\n\nloss, accuracy = model3.evaluate(test_dataset)\n\nprint(f\"Loss for the test set: {loss}\")\nprint(f\"Accuracy for the test set: {accuracy}\")\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n225/225 [==============================] - 2s 11ms/step - loss: 0.0465 - accuracy: 0.9891\nLoss on the test set: 0.04651045799255371\nAccuracy on the test set: 0.989086389541626\n\n\n\n\nEmbedding Visualization\nNow we are going to visualize and comment on the embedding of the model.\n# get the weights from the embedding layer\nweights = model3.get_layer('title_embedding').get_weights()[0]\n\n# Get the vocabulary from the text_vectorization layer\nvocab = title_vectorize_layer.get_vocabulary()  \n\n# Perform PCA on the embeddings\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\n# Create a DataFrame for the reduced embeddings\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': weights[:, 0],\n    'x1': weights[:, 1]\n})\n\n# Create a scatter plot\nfig = px.scatter(embedding_df,\n                 x=\"x0\",\n                 y=\"x1\",\n                 size_max = 5,\n                 hover_name=\"word\")\n\nfig.show()\nThe word “video” is located at the very right side of the x-axis, which could mean that it is a strong feature when determining fake news.\nSimilarly, the word “trump”, “obama”, and “hillary” are also located at the far end of the x-axis, implying that articles mentioning politician’s name could play a significant role in classifying the news.\n“mass”, “several”, “send” are located right in the middle. These words implies that they have less discriminative power in distinguishing between real and fake news. These words are used relatively equally in both fake and real news.\n“frances”, “zimbabwes”, and “myanmar” in the bottom left corner of the plot suggests they are outliers. They might appear only in specific types of articles, such as international or politic news.\n“fbi”, “die”, and “federal” are clustered near the top center of the plot. This might suggest that they have some discriminative power, due to their association with more formal or serious news content."
  },
  {
    "objectID": "posts/untitled folder/index.html",
    "href": "posts/untitled folder/index.html",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "",
    "text": "In this blog post, I will show you how to web scrape but I am going to create my own movie scraper. I will going to create TmdbSpider class which will scrape all the casts and the movies that the casts were in. To do so, I will create 3 different parse and the output will be saved in a seperate csv file."
  },
  {
    "objectID": "posts/untitled folder/index.html#setup-and-overview",
    "href": "posts/untitled folder/index.html#setup-and-overview",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Setup and Overview",
    "text": "Setup and Overview\nThe TMDB page we are going to use is https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/ which is Harry Potter and the Philosopher’s Stone.\nWe will first look through how the website looks like. Once you choose the movie, you can find the Full Cast & Crew link. Yhis will lead to page\n(original_url)cast/\nIf you scroll down, you will see the Cast section. If you click one of those, Alan Rickman, for example, then the full URL is going to be\nhttps://www.themoviedb.org/person/4566-alan-rickman\nOnce you get into the actors’s page, you will see the list of the actors’ acting, crew, production, (etc..) list."
  },
  {
    "objectID": "posts/untitled folder/index.html#write-the-scraper",
    "href": "posts/untitled folder/index.html#write-the-scraper",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Write the Scraper",
    "text": "Write the Scraper\nNow we will write a scraper using the scrapy.\nThe basic format of the scraper look like :\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nAlong here, we need to implement three parsing methods for the TmdbSpider class, which are parse(self, response), parse_full_credits(self, response), parse_actor_page(self, response).\n\nparse(self, response)\n\nInstruction :\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n    \"\"\"\n    Construct the full cast URL and navigate to the cast URL.\n    \"\"\"\n    # Make the full cast URL by adding \"/cast\" \n    full_cast_url = response.url + \"/cast\"\n    \n    # Yield a new scrapy.Request for the cast URL, and use the callback method to handle the response\n    yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n\nThe first method is pretty clear. We start from the movie main page. What this method have to do is to navigate to the Full Cast & Crew page. Once you click the page in the website, you can find out the page URL is just\n(original_url)cast/\nTherefore what we need to do here is to just add cast/ to the initial URL we have and yield a Request by spider’s callback. We can specify what should happen when we get there as a callback, which is to yielded scrapy.Request.\n\n\n\ndef parse_full_credits(self, response)\n\nInstruction :\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Select only the cast section, excluding crew, and extracts URLs to actor pages.\n    Navigates to each actor's page by using for loop.\n    \"\"\"\n    # Use selectors to extract only the cast members, which does not have class 'crew'\n    cast = response.css('ol.people.credits:not(.crew)')\n    \n    # Extract the 'href' links to the individual actor pages\n    hrefs = cast.css('div.info a::attr(href)').getall()\n\n    # Iterate for each actor page\n    for link in hrefs:\n        yield scrapy.Request(response.urljoin(link), callback=self.parse_actor_page)\n\nNow this method starts from the Full Cast & Crew page. We first need to request the page or each actor(only actor, not crew). Here we are going to use the selector. Each actor’s selector is in this format:\n&lt;div class=\"info\"&gt;\n            &lt;p&gt;&lt;a href=\"/person/194-richard-harris\"&gt;Richard Harris&lt;/a&gt;&lt;/p&gt;&lt;p&gt;\n            &lt;/p&gt;&lt;p class=\"character\"&gt;Albus Dumbledore\n            &lt;/p&gt;\n          &lt;/div&gt;\nHowever, not only actor has this format, but also crew has this format. So we have to look for higher class. Then we can find that the casting is under\n&lt;ol class=\"people credits \"&gt;\nand the crew is under\n&lt;ol class=\"people credits crew\"&gt;\nSo I first set the\ncast = response.css('ol.people.credits:not(.crew)') \nand then set\nhrefs = cast.css('div.info a::attr(href)').getall().\nsince what we see is in a hyperlink format, I wrote as a::attr(href) under div class info. I used getall() because we need to navigate to all the cast.\nThe for loop is to navigate each of the cast’s page and here request and callback self.parse_actor_page.\n\n\n\ndef parse_actor_page(self, response):\n\nInstruction :\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {“actor” : actor_name, “movie_or_TV_name” : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an “Acting” role1. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Extract the actor's name on the actor's page.\n    Extract movie's name that the actor had an acting role \n    by selecting the table that follows after the \"Acting\".\n    Create a set to avoid duplicates and yields actor's name and movie's name.\n    \"\"\"\n    # Extract actor's name from h2 tag class title\n    actor_name = response.css('h2.title ::text').get()\n\n    # Extract the table that follows the h3\n    acting_section = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[1]\")\n\n    # Create a set to store movie titles.\n    unique_movies = set()\n\n    for movie in acting_section.css(\"tr\"):\n        # Extract the text, which is the movie name\n        movie_name = movie.css('td a ::text').get()  \n\n        # Add the movie name to the set if it is not already in the set\n        if movie_name not in unique_movies:\n            unique_movies.add(movie_name)  \n\n            yield {\n                'actor': actor_name,\n                'movie_or_TV_name': movie_name\n            }\n\nWe start from actor’s page. First we will going to extract the actor’s name by calling\nactor_name = response.css('h2.title ::text').get()\nThe name is in h2.title format. So we extract just the text by ::text.\nNext we need to find what movie they acted in. We should only yield movies or TV shows on which that actor has worked in an “Acting” role. However, “Acting”, “crew”, “Production” are all under same  format:\n&lt;tbody&gt;&lt;tr&gt;\n          &lt;td&gt;\n            &lt;table class=\"credit_group\"&gt;\n              &lt;tbody&gt;&lt;tr&gt;\n                &lt;td class=\"year\"&gt;—&lt;/td&gt;\n                  &lt;td class=\"seperator\"&gt;&lt;span data-url=\"/tv/44337\" data-id=\"52596773760ee346619c97e4\" data-type=\"tv\" data-slug=\"44337\" class=\"glyphicons_v2 circle-empty account_adult_false item_adult_false\"&gt;&lt;/span&gt;&lt;/td&gt;\n                &lt;td class=\"role true account_adult_false item_adult_false\"&gt;\n                  &lt;a class=\"tooltip\" href=\"/tv/44337\"&gt;&lt;bdi&gt;Have I Got a Bit More News for You&lt;/bdi&gt;&lt;/a&gt;\n                    &lt;span class=\"group\"&gt; &lt;span&gt;(&lt;a class=\"tv\" href=\"/tv/44337/episodes?credit_id=5bce0b02c3a3683d6f000fd7&amp;person_id=4bc89155017a3c122d00c255\"&gt;2 episodes&lt;/a&gt;)&lt;/span&gt; as &lt;span class=\"character\"&gt;Self - Presenter&lt;/span&gt;&lt;/span&gt;\n                &lt;/td&gt;\n              &lt;/tr&gt;\n            &lt;/tbody&gt;&lt;/table&gt;\n          &lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/tbody&gt;\n    \nThe only thing that is different is each acting, crew or production are under the different h3 class zero title.\n&lt;h3 class=\"zero\"&gt;Acting&lt;/h3&gt;\n&lt;h3 class=\"zero\"&gt;Crew&lt;/h3&gt;\n&lt;h3 class=\"zero\"&gt;Production&lt;/h3&gt;\nHowever, the tables are not under these selectors, but the tables follow right after the titles. So we can extract the ‘Acting’ section by selecting the table right after this class.\nacting_section = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[1]\")\nThe code above is first filter the previously selected h3 elements. It selects only those content that contains the word “Acting”. And then find siblings that come after the selected node. Then select the first table element, table[1], which is a following sibling of the h3 containing the word “Acting”.\nIn this way we can get the movie name of only “Acting”."
  },
  {
    "objectID": "posts/untitled folder/index.html#run-the-scraper",
    "href": "posts/untitled folder/index.html#run-the-scraper",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Run the Scraper",
    "text": "Run the Scraper\nNow we can run the following command in the terminal inside the directory you want.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nThis command will make a csv file named results in the folder you want."
  },
  {
    "objectID": "posts/untitled folder/index.html#make-the-recommendations",
    "href": "posts/untitled folder/index.html#make-the-recommendations",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Make the Recommendations",
    "text": "Make the Recommendations\nBased on the data of actors and the movie they acted in, we can make a recommandation of the specific movie(here it is Harry Potter and the Philosopher’s Stone).\n\nimport pandas as pd\n\n# Load the CSV file\nresults_df = pd.read_csv('/Users/gimdong-gyu/Desktop/TMDB_scraper/results.csv')\n\nresults_df\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nDaniel Radcliffe\nMulligan\n\n\n4\nDaniel Radcliffe\nDigman!\n\n\n...\n...\n...\n\n\n2917\nRupert Grint\nThe View\n\n\n2918\nRupert Grint\nGMTV\n\n\n2919\nRupert Grint\nThe Tonight Show with Jay Leno\n\n\n2920\nRupert Grint\nAn Audience with...\n\n\n2921\nRupert Grint\nToday\n\n\n\n\n2922 rows × 2 columns\n\n\n\nWe can see there are 2922 rows, which means there are total 2922 movies that every cast had a roll in.\nBut we need to group those actors that were in the same movie and count them in order to see which movie has the most shared actor. Also we can make a recommandation based on the most shared movies.\n\n# group by the 'movie_or_TV_name' and count the number of unique actors.\nshared_actors = results_df.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\nshared_actors.columns = ['movie names', 'number of shared actors']\n\n# Sort by the number of shared actors \nshared_sorted = shared_actors.sort_values(by='number of shared actors', ascending=False)\nshared_sorted\n\n\n\n\n\n\n\n\nmovie names\nnumber of shared actors\n\n\n\n\n700\nHarry Potter and the Philosopher's Stone\n63\n\n\n694\nHarry Potter and the Chamber of Secrets\n37\n\n\n382\nCreating the World of Harry Potter\n36\n\n\n701\nHarry Potter and the Prisoner of Azkaban\n26\n\n\n699\nHarry Potter and the Order of the Phoenix\n24\n\n\n...\n...\n...\n\n\n796\nIndiana Jones and the Kingdom of the Crystal S...\n1\n\n\n795\nIndian Summers\n1\n\n\n794\nIn the Red\n1\n\n\n793\nIn the Heart of the Sea\n1\n\n\n2273\nZastrozzi: A Romance\n1\n\n\n\n\n2274 rows × 2 columns\n\n\n\nNow we want to visualize the top 10 movies that shows the most shared actors. To do this, We can use matplotlib to create a bar chart\n\nimport matplotlib.pyplot as plt\n\n# Get the top 10 movies/TV shows\ntop10 = shared_sorted.head(10)\n\n# Plotting the bar chart\nplt.figure(figsize=(10, 6))\nplt.barh(top10['movie names'], top10['number of shared actors'])\nplt.title('Top 10 Movies with the Most Shared Actors')\nplt.xlabel('Shared Actors Count')\nplt.ylabel('Movie')\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph, we can see that the Harry Potter and the Chamber of Secrets is the movie that has most shared actors with Harry Potter and the Philosopher’s Stone.\nHowever, this recommandation is not really useful because except for one movie, those movies in top 10 are all Harry Potter Series.\nTherefore We will make a seperate recommandation plot that does not contain Harry Potter Series.\n\nRecommandation Other than Harry Potter Series\n\n# Remove movies that contain \"Harry Potter\" in the title.\nno_harry = shared_sorted[~shared_sorted['movie names'].str.contains('Harry Potter')]\n\ntop10 = no_harry.head(10)\n\nplt.figure(figsize=(10, 6))\nplt.barh(top10['movie names'], top10['number of shared actors'])\nplt.title('Top 10 Movies with the Most Shared Actors (Other than Harry Potter)')\nplt.xlabel('Shared Actors Count')\nplt.ylabel('Movie')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plot, now we can see that the most most shared actor movie is Doctor Who and all the movies in the top 10 are now movies other than Harry Potter."
  },
  {
    "objectID": "posts/untitled folder/index.html#conclusion",
    "href": "posts/untitled folder/index.html#conclusion",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we looked through how we can apply scrapy to make a recommandation system. Web Scraping is a powerful tool to gather various data from the web."
  },
  {
    "objectID": "posts/hw3/index.html",
    "href": "posts/hw3/index.html",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "",
    "text": "URL for the Github repo : https://github.com/askpl00/flask_hw3_16B/tree/main"
  },
  {
    "objectID": "posts/hw3/index.html#overview-of-the-message-bank-web",
    "href": "posts/hw3/index.html#overview-of-the-message-bank-web",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "Overview of the Message Bank Web",
    "text": "Overview of the Message Bank Web\nThere are two big features in my web app.\n\nMessage Submission : Users can submit messages with their names.\nMessage Viewing : Users can view their provious subimitted messages."
  },
  {
    "objectID": "posts/hw3/index.html#the-main-page-route",
    "href": "posts/hw3/index.html#the-main-page-route",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "The Main Page : route(‘/’)",
    "text": "The Main Page : route(‘/’)\nTo start with, I set the main page of the website. The route (/) is the entry point, which is the first page a user sees when they visit the web app.\n@app.route('/')\ndef index():\n    return render_template('base.html')\nWhen a visitor goes to the home page, Flask executes the index function. This function uses Flask’s @app.route to tell Flask which URL should trigger our function. Then it renders base.html whuch also contains the navigation and layout."
  },
  {
    "objectID": "posts/hw3/index.html#base.html",
    "href": "posts/hw3/index.html#base.html",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "base.html",
    "text": "base.html\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} A Simple Message Bank&lt;/title&gt;\n&lt;nav&gt;   # Navigation section of the webpage\n  &lt;h1&gt;A Simple Message Bank&lt;/h1&gt;  # The site's main title or heading (same with the title above)\n  &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n  &lt;ul&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a message&lt;/a&gt;&lt;/li&gt;  # Navigation link to the page for submitting messages\n    &lt;li&gt;&lt;a href=\"{{ url_for('view')}}\"&gt;View messages&lt;/a&gt;&lt;/li&gt;  # Navigation link to the page for viewing messages \n\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\nIn base.html, a navigation bar is established, providing users to submit and view messages. It also sets a title, “A Simple Message Bank”. Page sections for content and headers are defined, and it enables to navigate link to the page for submitting messages or for viewing messages."
  },
  {
    "objectID": "posts/hw3/index.html#message-submission-page-routesubmit",
    "href": "posts/hw3/index.html#message-submission-page-routesubmit",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "Message Submission Page : route(‘/submit’)",
    "text": "Message Submission Page : route(‘/submit’)\nThe message submission feature consists a simple form where users can enter their name and message.\nThe submit view function allows users to submit messages. It responds to both GET and POST requests. When the route receives a GET request, it renders the submit.html template, which contains a form for the user to submit their message. For a POST request, which happens when a user submits the form, it processes the submitted data by calling insert_message(request) and renders the template with a message.\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    if request.method == 'POST':\n        # If submitted, process the submitted data\n        insert_message(request)\n        return render_template('submit.html', thanks=True)\n    # If the page is requested just as GET, just show the form\n    return render_template('submit.html')\n\n\n\nUser submitting a message"
  },
  {
    "objectID": "posts/hw3/index.html#insert_message-function",
    "href": "posts/hw3/index.html#insert_message-function",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "insert_message function",
    "text": "insert_message function\nThe insert_message function extracts the submitted message and the user’s name and inserts them into database.\ndef insert_message(request):\n\n    db = get_message_db()\n    cursor = db.cursor()\n    \n    # Extract the message and the handle from the given data\n    handle = request.form['handle']\n    message = request.form['message']\n    \n    # Insert the new message into the messages table\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    db.commit()\n    cursor.close()  # Close the cursor\ncursor.execute() prepares an SQL statement to insert the new message into the messages table. It tells the database to add a new message and uses handle and message from the user’s input, and insert them into the database."
  },
  {
    "objectID": "posts/hw3/index.html#get_message_db-function",
    "href": "posts/hw3/index.html#get_message_db-function",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "get_message_db function",
    "text": "get_message_db function\nThis function gets a connection to the database with the messages. This function tries to return database connection from Flask’s g object. If it doesn’t exist, it makes a new connection and ensures our messages table is created.\ndef get_message_db():\n\n    try:\n        # Try to return the database connection\n        return g.message_db\n    except:\n        # If it doesn't exist, create a new database \n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        \n        # Command to create a messages table if it does not exist\n        cmd = \"\"\"\n        CREATE TABLE IF NOT EXISTS messages (\n            id INTEGER PRIMARY KEY,\n            handle TEXT,\n            message TEXT\n        )\n        \"\"\"\n        \n        # Create a cursor to execute the SQL command\n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        \n        # Return the connection\n        return g.message_db\nIf a connection to the messages database doesn’t already exist within g, the function establishes a new SQLite connection to “messages_db.sqlite”. Additionally, it ensures the structure of the database is prepared for use by creating a messages table. Therefore, after running the code, it returns sqlite3 connection to the messages database."
  },
  {
    "objectID": "posts/hw3/index.html#message-view-page-routeview",
    "href": "posts/hw3/index.html#message-view-page-routeview",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "Message View Page : route(‘/view’)",
    "text": "Message View Page : route(‘/view’)\nView route and the random_messages function allows to show random messages\n@app.route('/view')\ndef view():\n    msgs = random_messages(5)  # 5 random messages from the database\n    return render_template('view.html', messages=msgs)\nWhen a user navigates to the ‘/view’ URL, the view() function is called. The view route in a Flask application displays messages from the database. It uses random_messages(5) to fetch a random sample of 5 messages. These messages goes to render_template to be displayed on ‘view.html’, offering differnt messages and names."
  },
  {
    "objectID": "posts/hw3/index.html#random_messages-function",
    "href": "posts/hw3/index.html#random_messages-function",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "random_messages function",
    "text": "random_messages function\ndef random_messages(n):\n\n    db = get_message_db()\n    cursor = db.cursor()\n    \n    # Fetch n random messages from the database\n    # If there are fewer than n messages, fetch all available messages.\n    query = \"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?\"\n   \n    # Fetch the results of the query\n    cursor.execute(query, (n,))\n    random_msgs = cursor.fetchall()\n    \n    db.close()  # Close the database connection\n    return random_msgs\nThe random_messages function picks a certain number of messages randomly from a list saved in the database, showing different messages each time users refresh. It connects to the database, chooses messages randomly up to the number asked for, stops the connection to save resources, and then gives back the messages it picked.\n\n\n\nUser viewing messages"
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "HW1 : Visualizing Climate Data using Query & Database",
    "section": "",
    "text": "In this blog post, we’ll look through interesting dataset on global temperatures and explore various ways of visualize using Python, SQLite, and Plotly. We will create interactive visualizations that can help us understand worldwide temperature over time.\n\n# To properly show figures in your blog,\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/hw1/index.html#yearly-temperature-variation",
    "href": "posts/hw1/index.html#yearly-temperature-variation",
    "title": "HW1 : Visualizing Climate Data using Query & Database",
    "section": "1) Yearly Temperature Variation",
    "text": "1) Yearly Temperature Variation\nQuestion: How has the temperature variation (difference between the highest and lowest temperatures) changed over the years for a given country?\nTo answer this question, yearly_temperature_variation will calculate the yearly temperature variation for a specified country and range of years.\nThis is what the yearly_temperature_variation query function looks like :\n\nfrom climate_database import yearly_temperature_variation\nimport inspect\nprint(inspect.getsource(yearly_temperature_variation))\n\ndef yearly_temperature_variation(db_file, country, start_year, end_year):\n    \"\"\"\n    Yearly temperature variation (difference between max and min temperatures) \n    for a given country and start year and end year.\n    \n    Returns: Dataframe with two columns: Year and Variation,\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    query = f\"\"\"\n    SELECT T.Year, (MAX(T.Temp) - MIN(T.Temp)) AS Variation\n    FROM temperatures T\n    JOIN stations S ON T.ID = S.ID\n    JOIN countries C ON substr(T.ID, 1, 2) = C.[FIPS 10-4]\n    WHERE C.Name = ? AND T.Year BETWEEN ? AND ?\n    GROUP BY T.Year\n    ORDER BY T.Year;\n    \"\"\"\n    df = pd.read_sql_query(query, conn, params=(country, start_year, end_year))\n    conn.close()\n    return df\n\n\n\nWith the plot_yearly_temperature_variation, we can create a line plot showing the temperature variation of a specific country.\n\ndef plot_yearly_temperature_variation(db_file, country, start_year, end_year):\n    \"\"\"\n    Creates a line plot showing the yearly temperature variation.\n    \"\"\"\n    \n    df = yearly_temperature_variation(db_file, country, start_year, end_year)\n    fig = px.line(df, x='Year', y='Variation', \n                  title=f'Yearly Temperature Variation in {country}')\n    fig.update_xaxes(title_text='Year')\n    fig.update_yaxes(title_text='Temperature Variation (°C)')\n    return fig\n\n\nplot_yearly_temperature_variation(\"climate_data.db\", \"Japan\", 1960, 2000)"
  },
  {
    "objectID": "posts/hw1/index.html#extreme-temperature-on-specific-year-by-month",
    "href": "posts/hw1/index.html#extreme-temperature-on-specific-year-by-month",
    "title": "HW1 : Visualizing Climate Data using Query & Database",
    "section": "2) Extreme Temperature on Specific year by Month",
    "text": "2) Extreme Temperature on Specific year by Month\nQuestion : “What are the hottest and coldest temperatures for each month in a given year?”\nThe second query function is plot_extreme_temperatures(df, year) function which get the extreme temperature, the highest and the lowest, in the world when you put the input year. It will show you facet plot with the lowest and the highest temperatre plots in seperate figure.\n\nfrom climate_database import extreme_temperatures\nimport inspect\nprint(inspect.getsource(extreme_temperatures))\n\ndef extreme_temperatures(db_file, year):\n    \"\"\"\n    Lowest and highest temperatures for each month in specific year.\n    \n    Returns: Dataframe containing the lowest and highest temperatures for each month \n    in the world.\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cmd = f\"\"\"\n    SELECT {year} AS Year, Month,\n           MIN(Temp) AS Min_Temperature,\n           MAX(Temp) AS Max_Temperature\n    FROM temperatures\n    WHERE Year = '{year}'\n    GROUP BY Month\n    ORDER BY Month\n    \"\"\"\n    df = pd.read_sql_query(cmd, conn)\n    # Prepare the DataFrame for faceting by melting it\n    df_melted = df.melt(id_vars=['Month'], value_vars=['Min_Temperature', 'Max_Temperature'],\n                        var_name='Temperature_Type', value_name='Temperature')\n    conn.close()\n    return df_melted\n\n\n\n\nimport plotly.express as px\n\ndef plot_extreme_temperatures(df, year):\n    \"\"\"\n    Creates a faceted line plot showing the lowest and highest temperatures for each month.\n    \n    Parameters:\n    - df (DataFrame): DataFrame containing the lowest and highest temperatures for each month, \n        which we got from the query.\n    - year (int): The year for which extreme temperatures are visualized.\n    \"\"\"\n    # Use the DataFrame directly for plotting with Plotly Express\n    fig = px.line(df, x='Month', y='Temperature', color='Temperature_Type', \n                  facet_col='Temperature_Type', title=f'Extreme Temperatures in {year}')\n    \n    # Update layout for better readability\n    fig.update_layout(xaxis_title='Month', yaxis_title='Temperature (°C)')\n    fig.show()\n\n\n# Plot using the year you want \n# Ex. year 2000\nplot_extreme_temperatures(extreme_temperatures(\"climate_data.db\", 2000), 2000)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BlogPic16B",
    "section": "",
    "text": "HW5 : Text Classification : Spotting Fake News\n\n\n\n\n\n\nweek 8\n\n\nweek 9\n\n\nHW\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW3 : Message Bank Web Development Using Flask\n\n\n\n\n\n\nweek 4\n\n\nweek 5\n\n\nHW\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW2 : Scrapying TMDB Website & Movie Recommandation\n\n\n\n\n\n\nweek 3\n\n\nweek 4\n\n\nHW\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW1 : Visualizing Climate Data using Query & Database\n\n\n\n\n\n\nweek 2\n\n\nweek 3\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW0 Creating Post\n\n\n\n\n\n\nweek 1\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\nNo matching items"
  }
]