[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html",
    "href": "posts/hw0/HW0_post-new.html",
    "title": "HW0 Creating Post",
    "section": "",
    "text": "In this post, we will explore the dataset of penguins that captures various aspects of their physical characteristics. We will visualize the dataset and the explore the relationship between different characteristics by using the Plotly library. We will focus on capturing the differences in Culmen Length and Culmen Depth among penguins based on their sex and the place they live. We will create some visualization to see if there is any significant difference of Culmen Length and Culmen Depth between sex or where they live."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#introduction",
    "href": "posts/hw0/HW0_post-new.html#introduction",
    "title": "HW0 Creating Post",
    "section": "",
    "text": "In this post, we will explore the dataset of penguins that captures various aspects of their physical characteristics. We will visualize the dataset and the explore the relationship between different characteristics by using the Plotly library. We will focus on capturing the differences in Culmen Length and Culmen Depth among penguins based on their sex and the place they live. We will create some visualization to see if there is any significant difference of Culmen Length and Culmen Depth between sex or where they live."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#load-the-dataset",
    "href": "posts/hw0/HW0_post-new.html#load-the-dataset",
    "title": "HW0 Creating Post",
    "section": "Load the Dataset",
    "text": "Load the Dataset\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\n\n# Load the dataset into a pandas DataFrame\npenguins = pd.read_csv(url)\n\n# First few rows of the data\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN"
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-sex",
    "href": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-sex",
    "title": "HW0 Creating Post",
    "section": "Visualizing Culmen Size by Sex",
    "text": "Visualizing Culmen Size by Sex\nFirst, we will going to see if there is a difference of Culmen Length and Depth between male and female penguins. We will plot the data in 2D plots with different scatter colors using the plotly library.\n\nimport plotly\nfrom plotly import express as px\n\nfig1 = px.scatter(data_frame = penguins,      \n                 x = \"Culmen Length (mm)\",    # column for x axis\n                 y = \"Culmen Depth (mm)\",     # column for y axis\n                 color = \"Sex\",               # column for dot color : \n                                              #   visualize the difference by sex\n                 width = 500,                 # width of figure\n                 height = 300,                # height of figure\n                 opacity = 0.5                # opacity of figure\n                )\n\n#reduce whitespace and add title\nfig1.update_layout(margin={\"r\":0, \"t\":50, \"l\":0, \"b\":0}, \n                  title = \"Culmen Length and Depth by Sex\")\nfig1.show()\n\n# Save the figure as HTML\nfrom plotly.io import write_html\nwrite_html(fig1, \"plot1.html\")\n\n                                                \n\n\nAs you can see above, overall, male penguin have deeper and longer Culmen. However, there is no clear boundary between the sex. That is because the length and the depth can be affected by other factors, especially the species of penguins."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-habitat",
    "href": "posts/hw0/HW0_post-new.html#visualizing-culmen-size-by-habitat",
    "title": "HW0 Creating Post",
    "section": "Visualizing Culmen Size by Habitat",
    "text": "Visualizing Culmen Size by Habitat\nNext, we’re curious about how the habitat of a penguin might influence its Culmen size. We will again plot the 2D scatter plot using the ploty library.\n\nfig2 = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",   \n                 y = \"Culmen Depth (mm)\",    \n                 color = \"Island\",          # column for dot color : \n                                            #   visualize the difference by habitat\n                 width = 500,\n                 height = 300,\n                 opacity = 0.5\n                )\n\nfig2.update_layout(margin={\"r\":0, \"t\":50, \"l\":0, \"b\":0}, \n                  title = \"Culmen Length and Depth by Habitat\")\nfig2.show()\n\nwrite_html(fig2, \"plot2.html\")\n\n                                                \n\n\nIn the plot, we can see that overall, the penguins in Drean island have the biggest Culmen. In Torgersen, those with similar Culmen live. However, in Biscoe and Dream island penguins with various size of Culmen live together."
  },
  {
    "objectID": "posts/hw0/HW0_post-new.html#conclusion",
    "href": "posts/hw0/HW0_post-new.html#conclusion",
    "title": "HW0 Creating Post",
    "section": "Conclusion",
    "text": "Conclusion\nThrough the visualiazation, we have observed the differences in Culmen size among penguins based on sex and their living islands. However, it looks like there isn’t clear and significant difference of the Culmen size by sex or habitat itself. We might need to add some more classifiers in order to detect the difference in the Culmen size."
  },
  {
    "objectID": "posts/hw3/index.html",
    "href": "posts/hw3/index.html",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "",
    "text": "URL for the Github repo : https://github.com/askpl00/flask_hw3_16B/tree/main"
  },
  {
    "objectID": "posts/hw3/index.html#overview-of-the-message-bank-web",
    "href": "posts/hw3/index.html#overview-of-the-message-bank-web",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "Overview of the Message Bank Web",
    "text": "Overview of the Message Bank Web\nThere are two big features in my web app.\n\nMessage Submission : Users can submit messages with their names.\nMessage Viewing : Users can view their provious subimitted messages."
  },
  {
    "objectID": "posts/hw3/index.html#the-main-page-route",
    "href": "posts/hw3/index.html#the-main-page-route",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "The Main Page : route(‘/’)",
    "text": "The Main Page : route(‘/’)\nTo start with, I set the main page of the website. The route (/) is the entry point, which is the first page a user sees when they visit the web app.\n@app.route('/')\ndef index():\n    return render_template('base.html')\nWhen a visitor goes to the home page, Flask executes the index function. This function uses Flask’s @app.route to tell Flask which URL should trigger our function. Then it renders base.html whuch also contains the navigation and layout."
  },
  {
    "objectID": "posts/hw3/index.html#base.html",
    "href": "posts/hw3/index.html#base.html",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "base.html",
    "text": "base.html\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} A Simple Message Bank&lt;/title&gt;\n&lt;nav&gt;   # Navigation section of the webpage\n  &lt;h1&gt;A Simple Message Bank&lt;/h1&gt;  # The site's main title or heading (same with the title above)\n  &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n  &lt;ul&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a message&lt;/a&gt;&lt;/li&gt;  # Navigation link to the page for submitting messages\n    &lt;li&gt;&lt;a href=\"{{ url_for('view')}}\"&gt;View messages&lt;/a&gt;&lt;/li&gt;  # Navigation link to the page for viewing messages \n\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\nIn base.html, a navigation bar is established, providing users to submit and view messages. It also sets a title, “A Simple Message Bank”. Page sections for content and headers are defined, and it enables to navigate link to the page for submitting messages or for viewing messages."
  },
  {
    "objectID": "posts/hw3/index.html#message-submission-page-routesubmit",
    "href": "posts/hw3/index.html#message-submission-page-routesubmit",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "Message Submission Page : route(‘/submit’)",
    "text": "Message Submission Page : route(‘/submit’)\nThe message submission feature consists a simple form where users can enter their name and message.\nThe submit view function allows users to submit messages. It responds to both GET and POST requests. When the route receives a GET request, it renders the submit.html template, which contains a form for the user to submit their message. For a POST request, which happens when a user submits the form, it processes the submitted data by calling insert_message(request) and renders the template with a message.\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    if request.method == 'POST':\n        # If submitted, process the submitted data\n        insert_message(request)\n        return render_template('submit.html', thanks=True)\n    # If the page is requested just as GET, just show the form\n    return render_template('submit.html')\n\n\n\nUser submitting a message"
  },
  {
    "objectID": "posts/hw3/index.html#insert_message-function",
    "href": "posts/hw3/index.html#insert_message-function",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "insert_message function",
    "text": "insert_message function\nThe insert_message function extracts the submitted message and the user’s name and inserts them into database.\ndef insert_message(request):\n\n    db = get_message_db()\n    cursor = db.cursor()\n    \n    # Extract the message and the handle from the given data\n    handle = request.form['handle']\n    message = request.form['message']\n    \n    # Insert the new message into the messages table\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    db.commit()\n    cursor.close()  # Close the cursor\ncursor.execute() prepares an SQL statement to insert the new message into the messages table. It tells the database to add a new message and uses handle and message from the user’s input, and insert them into the database."
  },
  {
    "objectID": "posts/hw3/index.html#get_message_db-function",
    "href": "posts/hw3/index.html#get_message_db-function",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "get_message_db function",
    "text": "get_message_db function\nThis function gets a connection to the database with the messages. This function tries to return database connection from Flask’s g object. If it doesn’t exist, it makes a new connection and ensures our messages table is created.\ndef get_message_db():\n\n    try:\n        # Try to return the database connection\n        return g.message_db\n    except:\n        # If it doesn't exist, create a new database \n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        \n        # Command to create a messages table if it does not exist\n        cmd = \"\"\"\n        CREATE TABLE IF NOT EXISTS messages (\n            id INTEGER PRIMARY KEY,\n            handle TEXT,\n            message TEXT\n        )\n        \"\"\"\n        \n        # Create a cursor to execute the SQL command\n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        \n        # Return the connection\n        return g.message_db\nIf a connection to the messages database doesn’t already exist within g, the function establishes a new SQLite connection to “messages_db.sqlite”. Additionally, it ensures the structure of the database is prepared for use by creating a messages table. Therefore, after running the code, it returns sqlite3 connection to the messages database."
  },
  {
    "objectID": "posts/hw3/index.html#message-view-page-routeview",
    "href": "posts/hw3/index.html#message-view-page-routeview",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "Message View Page : route(‘/view’)",
    "text": "Message View Page : route(‘/view’)\nView route and the random_messages function allows to show random messages\n@app.route('/view')\ndef view():\n    msgs = random_messages(5)  # 5 random messages from the database\n    return render_template('view.html', messages=msgs)\nWhen a user navigates to the ‘/view’ URL, the view() function is called. The view route in a Flask application displays messages from the database. It uses random_messages(5) to fetch a random sample of 5 messages. These messages goes to render_template to be displayed on ‘view.html’, offering differnt messages and names."
  },
  {
    "objectID": "posts/hw3/index.html#random_messages-function",
    "href": "posts/hw3/index.html#random_messages-function",
    "title": "HW3 : Message Bank Web Development Using Flask",
    "section": "random_messages function",
    "text": "random_messages function\ndef random_messages(n):\n\n    db = get_message_db()\n    cursor = db.cursor()\n    \n    # Fetch n random messages from the database\n    # If there are fewer than n messages, fetch all available messages.\n    query = \"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?\"\n   \n    # Fetch the results of the query\n    cursor.execute(query, (n,))\n    random_msgs = cursor.fetchall()\n    \n    db.close()  # Close the database connection\n    return random_msgs\nThe random_messages function picks a certain number of messages randomly from a list saved in the database, showing different messages each time users refresh. It connects to the database, chooses messages randomly up to the number asked for, stops the connection to save resources, and then gives back the messages it picked.\n\n\n\nUser viewing messages"
  },
  {
    "objectID": "posts/untitled folder/index.html",
    "href": "posts/untitled folder/index.html",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "",
    "text": "In this blog post, I will show you how to web scrape but I am going to create my own movie scraper. I will going to create TmdbSpider class which will scrape all the casts and the movies that the casts were in. To do so, I will create 3 different parse and the output will be saved in a seperate csv file."
  },
  {
    "objectID": "posts/untitled folder/index.html#setup-and-overview",
    "href": "posts/untitled folder/index.html#setup-and-overview",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Setup and Overview",
    "text": "Setup and Overview\nThe TMDB page we are going to use is https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/ which is Harry Potter and the Philosopher’s Stone.\nWe will first look through how the website looks like. Once you choose the movie, you can find the Full Cast & Crew link. Yhis will lead to page\n(original_url)cast/\nIf you scroll down, you will see the Cast section. If you click one of those, Alan Rickman, for example, then the full URL is going to be\nhttps://www.themoviedb.org/person/4566-alan-rickman\nOnce you get into the actors’s page, you will see the list of the actors’ acting, crew, production, (etc..) list."
  },
  {
    "objectID": "posts/untitled folder/index.html#write-the-scraper",
    "href": "posts/untitled folder/index.html#write-the-scraper",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Write the Scraper",
    "text": "Write the Scraper\nNow we will write a scraper using the scrapy.\nThe basic format of the scraper look like :\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nAlong here, we need to implement three parsing methods for the TmdbSpider class, which are parse(self, response), parse_full_credits(self, response), parse_actor_page(self, response).\n\nparse(self, response)\n\nInstruction :\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n    \"\"\"\n    Construct the full cast URL and navigate to the cast URL.\n    \"\"\"\n    # Make the full cast URL by adding \"/cast\" \n    full_cast_url = response.url + \"/cast\"\n    \n    # Yield a new scrapy.Request for the cast URL, and use the callback method to handle the response\n    yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n\nThe first method is pretty clear. We start from the movie main page. What this method have to do is to navigate to the Full Cast & Crew page. Once you click the page in the website, you can find out the page URL is just\n(original_url)cast/\nTherefore what we need to do here is to just add cast/ to the initial URL we have and yield a Request by spider’s callback. We can specify what should happen when we get there as a callback, which is to yielded scrapy.Request.\n\n\n\ndef parse_full_credits(self, response)\n\nInstruction :\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Select only the cast section, excluding crew, and extracts URLs to actor pages.\n    Navigates to each actor's page by using for loop.\n    \"\"\"\n    # Use selectors to extract only the cast members, which does not have class 'crew'\n    cast = response.css('ol.people.credits:not(.crew)')\n    \n    # Extract the 'href' links to the individual actor pages\n    hrefs = cast.css('div.info a::attr(href)').getall()\n\n    # Iterate for each actor page\n    for link in hrefs:\n        yield scrapy.Request(response.urljoin(link), callback=self.parse_actor_page)\n\nNow this method starts from the Full Cast & Crew page. We first need to request the page or each actor(only actor, not crew). Here we are going to use the selector. Each actor’s selector is in this format:\n&lt;div class=\"info\"&gt;\n            &lt;p&gt;&lt;a href=\"/person/194-richard-harris\"&gt;Richard Harris&lt;/a&gt;&lt;/p&gt;&lt;p&gt;\n            &lt;/p&gt;&lt;p class=\"character\"&gt;Albus Dumbledore\n            &lt;/p&gt;\n          &lt;/div&gt;\nHowever, not only actor has this format, but also crew has this format. So we have to look for higher class. Then we can find that the casting is under\n&lt;ol class=\"people credits \"&gt;\nand the crew is under\n&lt;ol class=\"people credits crew\"&gt;\nSo I first set the\ncast = response.css('ol.people.credits:not(.crew)') \nand then set\nhrefs = cast.css('div.info a::attr(href)').getall().\nsince what we see is in a hyperlink format, I wrote as a::attr(href) under div class info. I used getall() because we need to navigate to all the cast.\nThe for loop is to navigate each of the cast’s page and here request and callback self.parse_actor_page.\n\n\n\ndef parse_actor_page(self, response):\n\nInstruction :\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {“actor” : actor_name, “movie_or_TV_name” : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an “Acting” role1. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Extract the actor's name on the actor's page.\n    Extract movie's name that the actor had an acting role \n    by selecting the table that follows after the \"Acting\".\n    Create a set to avoid duplicates and yields actor's name and movie's name.\n    \"\"\"\n    # Extract actor's name from h2 tag class title\n    actor_name = response.css('h2.title ::text').get()\n\n    # Extract the table that follows the h3\n    acting_section = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[1]\")\n\n    # Create a set to store movie titles.\n    unique_movies = set()\n\n    for movie in acting_section.css(\"tr\"):\n        # Extract the text, which is the movie name\n        movie_name = movie.css('td a ::text').get()  \n\n        # Add the movie name to the set if it is not already in the set\n        if movie_name not in unique_movies:\n            unique_movies.add(movie_name)  \n\n            yield {\n                'actor': actor_name,\n                'movie_or_TV_name': movie_name\n            }\n\nWe start from actor’s page. First we will going to extract the actor’s name by calling\nactor_name = response.css('h2.title ::text').get()\nThe name is in h2.title format. So we extract just the text by ::text.\nNext we need to find what movie they acted in. We should only yield movies or TV shows on which that actor has worked in an “Acting” role. However, “Acting”, “crew”, “Production” are all under same  format:\n&lt;tbody&gt;&lt;tr&gt;\n          &lt;td&gt;\n            &lt;table class=\"credit_group\"&gt;\n              &lt;tbody&gt;&lt;tr&gt;\n                &lt;td class=\"year\"&gt;—&lt;/td&gt;\n                  &lt;td class=\"seperator\"&gt;&lt;span data-url=\"/tv/44337\" data-id=\"52596773760ee346619c97e4\" data-type=\"tv\" data-slug=\"44337\" class=\"glyphicons_v2 circle-empty account_adult_false item_adult_false\"&gt;&lt;/span&gt;&lt;/td&gt;\n                &lt;td class=\"role true account_adult_false item_adult_false\"&gt;\n                  &lt;a class=\"tooltip\" href=\"/tv/44337\"&gt;&lt;bdi&gt;Have I Got a Bit More News for You&lt;/bdi&gt;&lt;/a&gt;\n                    &lt;span class=\"group\"&gt; &lt;span&gt;(&lt;a class=\"tv\" href=\"/tv/44337/episodes?credit_id=5bce0b02c3a3683d6f000fd7&amp;person_id=4bc89155017a3c122d00c255\"&gt;2 episodes&lt;/a&gt;)&lt;/span&gt; as &lt;span class=\"character\"&gt;Self - Presenter&lt;/span&gt;&lt;/span&gt;\n                &lt;/td&gt;\n              &lt;/tr&gt;\n            &lt;/tbody&gt;&lt;/table&gt;\n          &lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/tbody&gt;\n    \nThe only thing that is different is each acting, crew or production are under the different h3 class zero title.\n&lt;h3 class=\"zero\"&gt;Acting&lt;/h3&gt;\n&lt;h3 class=\"zero\"&gt;Crew&lt;/h3&gt;\n&lt;h3 class=\"zero\"&gt;Production&lt;/h3&gt;\nHowever, the tables are not under these selectors, but the tables follow right after the titles. So we can extract the ‘Acting’ section by selecting the table right after this class.\nacting_section = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[1]\")\nThe code above is first filter the previously selected h3 elements. It selects only those content that contains the word “Acting”. And then find siblings that come after the selected node. Then select the first table element, table[1], which is a following sibling of the h3 containing the word “Acting”.\nIn this way we can get the movie name of only “Acting”."
  },
  {
    "objectID": "posts/untitled folder/index.html#run-the-scraper",
    "href": "posts/untitled folder/index.html#run-the-scraper",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Run the Scraper",
    "text": "Run the Scraper\nNow we can run the following command in the terminal inside the directory you want.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nThis command will make a csv file named results in the folder you want."
  },
  {
    "objectID": "posts/untitled folder/index.html#make-the-recommendations",
    "href": "posts/untitled folder/index.html#make-the-recommendations",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Make the Recommendations",
    "text": "Make the Recommendations\nBased on the data of actors and the movie they acted in, we can make a recommandation of the specific movie(here it is Harry Potter and the Philosopher’s Stone).\n\nimport pandas as pd\n\n# Load the CSV file\nresults_df = pd.read_csv('/Users/gimdong-gyu/Desktop/TMDB_scraper/results.csv')\n\nresults_df\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nDaniel Radcliffe\nMulligan\n\n\n4\nDaniel Radcliffe\nDigman!\n\n\n...\n...\n...\n\n\n2917\nRupert Grint\nThe View\n\n\n2918\nRupert Grint\nGMTV\n\n\n2919\nRupert Grint\nThe Tonight Show with Jay Leno\n\n\n2920\nRupert Grint\nAn Audience with...\n\n\n2921\nRupert Grint\nToday\n\n\n\n\n2922 rows × 2 columns\n\n\n\nWe can see there are 2922 rows, which means there are total 2922 movies that every cast had a roll in.\nBut we need to group those actors that were in the same movie and count them in order to see which movie has the most shared actor. Also we can make a recommandation based on the most shared movies.\n\n# group by the 'movie_or_TV_name' and count the number of unique actors.\nshared_actors = results_df.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\nshared_actors.columns = ['movie names', 'number of shared actors']\n\n# Sort by the number of shared actors \nshared_sorted = shared_actors.sort_values(by='number of shared actors', ascending=False)\nshared_sorted\n\n\n\n\n\n\n\n\nmovie names\nnumber of shared actors\n\n\n\n\n700\nHarry Potter and the Philosopher's Stone\n63\n\n\n694\nHarry Potter and the Chamber of Secrets\n37\n\n\n382\nCreating the World of Harry Potter\n36\n\n\n701\nHarry Potter and the Prisoner of Azkaban\n26\n\n\n699\nHarry Potter and the Order of the Phoenix\n24\n\n\n...\n...\n...\n\n\n796\nIndiana Jones and the Kingdom of the Crystal S...\n1\n\n\n795\nIndian Summers\n1\n\n\n794\nIn the Red\n1\n\n\n793\nIn the Heart of the Sea\n1\n\n\n2273\nZastrozzi: A Romance\n1\n\n\n\n\n2274 rows × 2 columns\n\n\n\nNow we want to visualize the top 10 movies that shows the most shared actors. To do this, We can use matplotlib to create a bar chart\n\nimport matplotlib.pyplot as plt\n\n# Get the top 10 movies/TV shows\ntop10 = shared_sorted.head(10)\n\n# Plotting the bar chart\nplt.figure(figsize=(10, 6))\nplt.barh(top10['movie names'], top10['number of shared actors'])\nplt.title('Top 10 Movies with the Most Shared Actors')\nplt.xlabel('Shared Actors Count')\nplt.ylabel('Movie')\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph, we can see that the Harry Potter and the Chamber of Secrets is the movie that has most shared actors with Harry Potter and the Philosopher’s Stone.\nHowever, this recommandation is not really useful because except for one movie, those movies in top 10 are all Harry Potter Series.\nTherefore We will make a seperate recommandation plot that does not contain Harry Potter Series.\n\nRecommandation Other than Harry Potter Series\n\n# Remove movies that contain \"Harry Potter\" in the title.\nno_harry = shared_sorted[~shared_sorted['movie names'].str.contains('Harry Potter')]\n\ntop10 = no_harry.head(10)\n\nplt.figure(figsize=(10, 6))\nplt.barh(top10['movie names'], top10['number of shared actors'])\nplt.title('Top 10 Movies with the Most Shared Actors (Other than Harry Potter)')\nplt.xlabel('Shared Actors Count')\nplt.ylabel('Movie')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plot, now we can see that the most most shared actor movie is Doctor Who and all the movies in the top 10 are now movies other than Harry Potter."
  },
  {
    "objectID": "posts/untitled folder/index.html#conclusion",
    "href": "posts/untitled folder/index.html#conclusion",
    "title": "HW2 : Scrapying TMDB Website & Movie Recommandation",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we looked through how we can apply scrapy to make a recommandation system. Web Scraping is a powerful tool to gather various data from the web."
  },
  {
    "objectID": "posts/hw5/index.html",
    "href": "posts/hw5/index.html",
    "title": "BlogPic16B",
    "section": "",
    "text": "import os\nfrom keras import utils\nimport tensorflow_datasets as tfds\n\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\n\nimport keras\n\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\nimport matplotlib.pyplot as plt\n\ndef visualization(train_ds):\n\n    plt.figure(figsize=(10, 5))\n    cat_images = []\n    dog_images = []\n\n    # Get first three images of cats and dogs\n    for images, labels in train_ds.unbatch().as_numpy_iterator():\n        if labels == 0 and len(cat_images) &lt; 3:\n            cat_images.append(images)\n        elif labels == 1 and len(dog_images) &lt; 3:\n            dog_images.append(images)\n\n    for i in range(3):  # First three cats\n        plt.subplot(2, 3, i + 1)\n        plt.imshow(cat_images[i].astype(\"uint8\"))\n        plt.axis(\"off\")\n        plt.title(\"Cat\")\n    for i in range(3):  # First three dogs\n        plt.subplot(2, 3, i + 4)  # Dogs start from the 4th position\n        plt.imshow(dog_images[i].astype(\"uint8\"))\n        plt.axis(\"off\")\n        plt.title(\"Dog\")\n\n    plt.show()\n\nvisualization(train_ds)\n\n\n\n\n\n\n\n\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncat_count = 0\ndog_count = 0\n\n# Iterate in the dataset and count cats and dogs\nfor label in labels_iterator:\n    if label == 0:\n        cat_count += 1\n    elif label == 1:\n        dog_count += 1\n\nprint(f\"Number of cat images: {cat_count}\")\nprint(f\"Number of dog images: {dog_count}\")\n\n# Calculate the baseline model accuracy\ntotal_images = cat_count + dog_count\nmost_frequent_label_count = max(cat_count, dog_count)\nbaseline_accuracy = most_frequent_label_count / total_images\n\nprint(f\"Baseline model accuracy: {baseline_accuracy:.2f}\")\n\nNumber of cat images: 4637\nNumber of dog images: 4668\nBaseline model accuracy: 0.50\n\n\n\nfrom keras import datasets, layers, models\n\nmodel1 = models.Sequential([\n\n    layers.Input((150, 150, 3)),\n\n    # First Convolutional Block\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Convolutional Block\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid')   # for binary classification\n])\n\n\n# Compile the model\nmodel1.compile(optimizer='adam',\n              loss='binary_crossentropy',    # for binary classification\n              metrics=['accuracy'])\n\nmodel1.summary()\n\n# Train the model\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\n# Plot the training and validation accuracy\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n flatten (Flatten)           (None, 82944)             0         \n                                                                 \n dense (Dense)               (None, 64)                5308480   \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_1 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 5327937 (20.32 MB)\nTrainable params: 5327937 (20.32 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/20\n146/146 [==============================] - 18s 70ms/step - loss: 28.7778 - accuracy: 0.5525 - val_loss: 0.6760 - val_accuracy: 0.5722\nEpoch 2/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6321 - accuracy: 0.6361 - val_loss: 0.6730 - val_accuracy: 0.5881\nEpoch 3/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5104 - accuracy: 0.7269 - val_loss: 0.7193 - val_accuracy: 0.5937\nEpoch 4/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.4314 - accuracy: 0.7783 - val_loss: 0.8652 - val_accuracy: 0.5830\nEpoch 5/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.3516 - accuracy: 0.8275 - val_loss: 1.0036 - val_accuracy: 0.6015\nEpoch 6/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2980 - accuracy: 0.8598 - val_loss: 1.1715 - val_accuracy: 0.6337\nEpoch 7/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.2789 - accuracy: 0.8848 - val_loss: 1.5469 - val_accuracy: 0.5985\nEpoch 8/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.2414 - accuracy: 0.8921 - val_loss: 1.3987 - val_accuracy: 0.6071\nEpoch 9/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1998 - accuracy: 0.9207 - val_loss: 1.3468 - val_accuracy: 0.6066\nEpoch 10/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.2025 - accuracy: 0.9237 - val_loss: 1.5451 - val_accuracy: 0.5929\nEpoch 11/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1972 - accuracy: 0.9265 - val_loss: 1.7485 - val_accuracy: 0.5933\nEpoch 12/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1592 - accuracy: 0.9425 - val_loss: 2.0177 - val_accuracy: 0.5997\nEpoch 13/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1185 - accuracy: 0.9599 - val_loss: 1.9909 - val_accuracy: 0.6019\nEpoch 14/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1056 - accuracy: 0.9607 - val_loss: 1.9737 - val_accuracy: 0.6015\nEpoch 15/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.0907 - accuracy: 0.9673 - val_loss: 2.3578 - val_accuracy: 0.6174\nEpoch 16/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.0813 - accuracy: 0.9707 - val_loss: 2.7964 - val_accuracy: 0.6071\nEpoch 17/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.0795 - accuracy: 0.9759 - val_loss: 2.4799 - val_accuracy: 0.6045\nEpoch 18/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.0685 - accuracy: 0.9759 - val_loss: 2.7452 - val_accuracy: 0.6023\nEpoch 19/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.0693 - accuracy: 0.9785 - val_loss: 2.4840 - val_accuracy: 0.6204\nEpoch 20/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.0517 - accuracy: 0.9838 - val_loss: 2.7648 - val_accuracy: 0.6109\n\n\n\n\n\n\n\n\n\n\nfor images, _ in train_ds.take(1):\n\n    original_image = images[50]\n    plt.figure(figsize=(12, 12))\n\n    # Original image\n    plt.subplot(4, 4, 1)\n    plt.imshow(original_image.numpy().astype(\"uint8\"))\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    # Random Flip\n    flip = layers.RandomFlip(\"horizontal_and_vertical\")\n    for i in range(2, 9):\n        plt.subplot(4, 4, i)\n        flip_image = flip(original_image, training=True)\n        plt.imshow(flip_image.numpy().astype(\"uint8\"))\n        plt.title(\"Flip\")\n        plt.axis(\"off\")\n\n    # Random Rotation\n    rotation = layers.RandomRotation(0.2)\n    for i in range(9, 17):\n        plt.subplot(4, 4, i)\n        rotate_image = rotation(original_image, training=True)\n        plt.imshow(rotate_image.numpy().astype(\"uint8\"))\n        plt.title(\"Rotation\")\n        plt.axis(\"off\")\n\n    plt.show()\n\n\n\n\n\n\n\n\n\nfrom tensorflow.keras import layers, models, optimizers, losses\n\nmodel2 = models.Sequential([\n\n    layers.Input((150, 150, 3)),\n\n    #RandomFlip() layer and a RandomRotation() layer\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid')\n\n])\n\nmodel2.compile(optimizer = 'adam',\n              loss=losses.BinaryCrossentropy(from_logits=False),\n              metrics = ['accuracy'])\n\nmodel2.summary()\n\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_1 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_1 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n conv2d_2 (Conv2D)           (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d_2 (MaxPoolin  (None, 74, 74, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_3 (Conv2D)           (None, 72, 72, 64)        18496     \n                                                                 \n max_pooling2d_3 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n flatten_1 (Flatten)         (None, 82944)             0         \n                                                                 \n dense_2 (Dense)             (None, 64)                5308480   \n                                                                 \n dropout_1 (Dropout)         (None, 64)                0         \n                                                                 \n dense_3 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 5327937 (20.32 MB)\nTrainable params: 5327937 (20.32 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/20\n146/146 [==============================] - 8s 38ms/step - loss: 31.7002 - accuracy: 0.5430 - val_loss: 0.6609 - val_accuracy: 0.5791\nEpoch 2/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6764 - accuracy: 0.5846 - val_loss: 0.6701 - val_accuracy: 0.5804\nEpoch 3/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6670 - accuracy: 0.5731 - val_loss: 0.6599 - val_accuracy: 0.6187\nEpoch 4/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6628 - accuracy: 0.5884 - val_loss: 0.6694 - val_accuracy: 0.5873\nEpoch 5/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6718 - accuracy: 0.5790 - val_loss: 0.6574 - val_accuracy: 0.6096\nEpoch 6/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.6585 - accuracy: 0.6124 - val_loss: 0.6539 - val_accuracy: 0.5993\nEpoch 7/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6421 - accuracy: 0.6343 - val_loss: 0.6362 - val_accuracy: 0.6285\nEpoch 8/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6518 - accuracy: 0.6232 - val_loss: 0.6345 - val_accuracy: 0.6341\nEpoch 9/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6433 - accuracy: 0.6336 - val_loss: 0.6198 - val_accuracy: 0.6531\nEpoch 10/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6276 - accuracy: 0.6469 - val_loss: 0.6193 - val_accuracy: 0.6617\nEpoch 11/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6197 - accuracy: 0.6502 - val_loss: 0.6208 - val_accuracy: 0.6531\nEpoch 12/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.6136 - accuracy: 0.6636 - val_loss: 0.6074 - val_accuracy: 0.6763\nEpoch 13/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6080 - accuracy: 0.6663 - val_loss: 0.6145 - val_accuracy: 0.6578\nEpoch 14/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6126 - accuracy: 0.6620 - val_loss: 0.5976 - val_accuracy: 0.6776\nEpoch 15/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6103 - accuracy: 0.6660 - val_loss: 0.6001 - val_accuracy: 0.6745\nEpoch 16/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6027 - accuracy: 0.6742 - val_loss: 0.5892 - val_accuracy: 0.6844\nEpoch 17/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6093 - accuracy: 0.6686 - val_loss: 0.6065 - val_accuracy: 0.6763\nEpoch 18/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6100 - accuracy: 0.6689 - val_loss: 0.5978 - val_accuracy: 0.6849\nEpoch 19/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6030 - accuracy: 0.6685 - val_loss: 0.5957 - val_accuracy: 0.6853\nEpoch 20/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6074 - accuracy: 0.6710 - val_loss: 0.5892 - val_accuracy: 0.6939\n\n\n\n\n\n\n\n\n\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\nmodel3 = models.Sequential([\n\n    preprocessor,\n\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n\n    # First Convolutional Block\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Convolutional Block\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Adding additional Convolutional Block to enhance the model\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    # Additional Dense layer for complexity\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel3.compile(optimizer = 'adam',\n              loss=losses.BinaryCrossentropy(from_logits=False),\n              metrics = ['accuracy'])\n\nmodel3.summary()\n\nhistory = model3.fit(train_ds,\n                      epochs=20,\n                      validation_data=validation_ds)\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model (Functional)          (None, 150, 150, 3)       0         \n                                                                 \n random_flip_2 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_2 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n conv2d_4 (Conv2D)           (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d_4 (MaxPoolin  (None, 74, 74, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_5 (Conv2D)           (None, 72, 72, 64)        18496     \n                                                                 \n max_pooling2d_5 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n conv2d_6 (Conv2D)           (None, 34, 34, 128)       73856     \n                                                                 \n max_pooling2d_6 (MaxPoolin  (None, 17, 17, 128)       0         \n g2D)                                                            \n                                                                 \n flatten_2 (Flatten)         (None, 36992)             0         \n                                                                 \n dense_4 (Dense)             (None, 128)               4735104   \n                                                                 \n dense_5 (Dense)             (None, 64)                8256      \n                                                                 \n dropout_2 (Dropout)         (None, 64)                0         \n                                                                 \n dense_6 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 4836673 (18.45 MB)\nTrainable params: 4836673 (18.45 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/20\n146/146 [==============================] - 10s 47ms/step - loss: 0.6571 - accuracy: 0.6048 - val_loss: 0.5811 - val_accuracy: 0.6943\nEpoch 2/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5950 - accuracy: 0.6846 - val_loss: 0.5476 - val_accuracy: 0.7206\nEpoch 3/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5700 - accuracy: 0.7020 - val_loss: 0.5358 - val_accuracy: 0.7326\nEpoch 4/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5577 - accuracy: 0.7130 - val_loss: 0.5183 - val_accuracy: 0.7365\nEpoch 5/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5440 - accuracy: 0.7236 - val_loss: 0.5162 - val_accuracy: 0.7412\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5261 - accuracy: 0.7342 - val_loss: 0.5177 - val_accuracy: 0.7412\nEpoch 7/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5160 - accuracy: 0.7422 - val_loss: 0.5068 - val_accuracy: 0.7549\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5051 - accuracy: 0.7509 - val_loss: 0.4928 - val_accuracy: 0.7683\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4862 - accuracy: 0.7656 - val_loss: 0.4933 - val_accuracy: 0.7678\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4811 - accuracy: 0.7673 - val_loss: 0.4745 - val_accuracy: 0.7794\nEpoch 11/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4702 - accuracy: 0.7760 - val_loss: 0.4845 - val_accuracy: 0.7687\nEpoch 12/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4578 - accuracy: 0.7802 - val_loss: 0.4796 - val_accuracy: 0.7721\nEpoch 13/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4536 - accuracy: 0.7866 - val_loss: 0.4598 - val_accuracy: 0.7923\nEpoch 14/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4391 - accuracy: 0.7959 - val_loss: 0.4418 - val_accuracy: 0.7945\nEpoch 15/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4298 - accuracy: 0.7999 - val_loss: 0.4554 - val_accuracy: 0.8005\nEpoch 16/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4163 - accuracy: 0.8089 - val_loss: 0.4331 - val_accuracy: 0.8035\nEpoch 17/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4111 - accuracy: 0.8113 - val_loss: 0.4206 - val_accuracy: 0.8044\nEpoch 18/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4110 - accuracy: 0.8119 - val_loss: 0.4224 - val_accuracy: 0.8156\nEpoch 19/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3887 - accuracy: 0.8219 - val_loss: 0.4275 - val_accuracy: 0.8190\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3814 - accuracy: 0.8250 - val_loss: 0.4193 - val_accuracy: 0.8237\n\n\n\n\n\n\n\n\n\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nmodel4 = models.Sequential([\n    base_model_layer,\n\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n\n    # Pooling layer to reduce dimensionality\n    layers.GlobalMaxPooling2D(),\n\n    layers.Dropout(0.2),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(2, activation='softmax')\n])\n\nmodel4.compile(optimizer='adam',\n               loss=losses.SparseCategoricalCrossentropy(),\n               metrics=['accuracy'])\n\nmodel4.summary()\n\nhistory = model4.fit(train_ds,\n                      epochs=20,\n                      validation_data=validation_ds)\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 1s 0us/step\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n random_flip_3 (RandomFlip)  (None, 5, 5, 960)         0         \n                                                                 \n random_rotation_3 (RandomR  (None, 5, 5, 960)         0         \n otation)                                                        \n                                                                 \n global_max_pooling2d (Glob  (None, 960)               0         \n alMaxPooling2D)                                                 \n                                                                 \n dropout_3 (Dropout)         (None, 960)               0         \n                                                                 \n dense_7 (Dense)             (None, 16)                15376     \n                                                                 \n dense_8 (Dense)             (None, 2)                 34        \n                                                                 \n=================================================================\nTotal params: 3011762 (11.49 MB)\nTrainable params: 15410 (60.20 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\nEpoch 1/20\n146/146 [==============================] - 16s 64ms/step - loss: 0.2915 - accuracy: 0.9270 - val_loss: 0.0856 - val_accuracy: 0.9669\nEpoch 2/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0885 - accuracy: 0.9666 - val_loss: 0.0703 - val_accuracy: 0.9712\nEpoch 3/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.0738 - accuracy: 0.9723 - val_loss: 0.0666 - val_accuracy: 0.9755\nEpoch 4/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0691 - accuracy: 0.9737 - val_loss: 0.0716 - val_accuracy: 0.9746\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0646 - accuracy: 0.9756 - val_loss: 0.0691 - val_accuracy: 0.9746\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0570 - accuracy: 0.9786 - val_loss: 0.0682 - val_accuracy: 0.9781\nEpoch 7/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0543 - accuracy: 0.9782 - val_loss: 0.0735 - val_accuracy: 0.9764\nEpoch 8/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.0544 - accuracy: 0.9776 - val_loss: 0.0759 - val_accuracy: 0.9755\nEpoch 9/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0506 - accuracy: 0.9815 - val_loss: 0.0762 - val_accuracy: 0.9755\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0467 - accuracy: 0.9815 - val_loss: 0.0759 - val_accuracy: 0.9742\nEpoch 11/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0480 - accuracy: 0.9813 - val_loss: 0.0778 - val_accuracy: 0.9746\nEpoch 12/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0459 - accuracy: 0.9826 - val_loss: 0.0797 - val_accuracy: 0.9746\nEpoch 13/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0393 - accuracy: 0.9844 - val_loss: 0.0779 - val_accuracy: 0.9768\nEpoch 14/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0492 - accuracy: 0.9814 - val_loss: 0.0910 - val_accuracy: 0.9699\nEpoch 15/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0442 - accuracy: 0.9842 - val_loss: 0.0832 - val_accuracy: 0.9776\nEpoch 16/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0413 - accuracy: 0.9830 - val_loss: 0.0801 - val_accuracy: 0.9759\nEpoch 17/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0390 - accuracy: 0.9834 - val_loss: 0.0892 - val_accuracy: 0.9742\nEpoch 18/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0359 - accuracy: 0.9855 - val_loss: 0.0973 - val_accuracy: 0.9716\nEpoch 19/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0392 - accuracy: 0.9844 - val_loss: 0.0858 - val_accuracy: 0.9725\nEpoch 20/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0338 - accuracy: 0.9867 - val_loss: 0.0888 - val_accuracy: 0.9712\n\n\n\n\n\n\n\n\n\n\nmodel5 = models.Sequential([\n    base_model_layer,\n\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n\n    # Pooling layer to reduce dimensionality\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.5),   # Changed to 0.5\n    layers.Dense(64, activation='relu'),  # Changed to 64\n    layers.Dense(2, activation='softmax')\n])\n\nmodel5.compile(optimizer='adam',\n               loss=losses.SparseCategoricalCrossentropy(),\n               metrics=['accuracy'])\n\nmodel5.summary()\n\nhistory = model5.fit(train_ds,\n                      epochs=20,\n                      validation_data=validation_ds)\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n random_flip_4 (RandomFlip)  (None, 5, 5, 960)         0         \n                                                                 \n random_rotation_4 (RandomR  (None, 5, 5, 960)         0         \n otation)                                                        \n                                                                 \n global_max_pooling2d_1 (Gl  (None, 960)               0         \n obalMaxPooling2D)                                               \n                                                                 \n dropout_4 (Dropout)         (None, 960)               0         \n                                                                 \n dense_9 (Dense)             (None, 64)                61504     \n                                                                 \n dense_10 (Dense)            (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 3057986 (11.67 MB)\nTrainable params: 61634 (240.76 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\nEpoch 1/20\n146/146 [==============================] - 13s 54ms/step - loss: 0.7425 - accuracy: 0.9170 - val_loss: 0.1128 - val_accuracy: 0.9643\nEpoch 2/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1864 - accuracy: 0.9490 - val_loss: 0.0897 - val_accuracy: 0.9682\nEpoch 3/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.1393 - accuracy: 0.9530 - val_loss: 0.0711 - val_accuracy: 0.9721\nEpoch 4/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1149 - accuracy: 0.9573 - val_loss: 0.0632 - val_accuracy: 0.9733\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0972 - accuracy: 0.9644 - val_loss: 0.0655 - val_accuracy: 0.9746\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0884 - accuracy: 0.9668 - val_loss: 0.0662 - val_accuracy: 0.9746\nEpoch 7/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0919 - accuracy: 0.9648 - val_loss: 0.0654 - val_accuracy: 0.9759\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0796 - accuracy: 0.9714 - val_loss: 0.0638 - val_accuracy: 0.9733\nEpoch 9/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0852 - accuracy: 0.9697 - val_loss: 0.0665 - val_accuracy: 0.9742\nEpoch 10/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0731 - accuracy: 0.9714 - val_loss: 0.0700 - val_accuracy: 0.9729\nEpoch 11/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0775 - accuracy: 0.9699 - val_loss: 0.0619 - val_accuracy: 0.9759\nEpoch 12/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0736 - accuracy: 0.9714 - val_loss: 0.0643 - val_accuracy: 0.9764\nEpoch 13/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.0776 - accuracy: 0.9693 - val_loss: 0.0618 - val_accuracy: 0.9772\nEpoch 14/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.0702 - accuracy: 0.9736 - val_loss: 0.0658 - val_accuracy: 0.9751\nEpoch 15/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0698 - accuracy: 0.9731 - val_loss: 0.0682 - val_accuracy: 0.9759\nEpoch 16/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0721 - accuracy: 0.9722 - val_loss: 0.0670 - val_accuracy: 0.9751\nEpoch 17/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0695 - accuracy: 0.9733 - val_loss: 0.0637 - val_accuracy: 0.9764\nEpoch 18/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0620 - accuracy: 0.9758 - val_loss: 0.0740 - val_accuracy: 0.9708\nEpoch 19/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0688 - accuracy: 0.9754 - val_loss: 0.0682 - val_accuracy: 0.9742\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0612 - accuracy: 0.9756 - val_loss: 0.0707 - val_accuracy: 0.9751\n\n\n\n\n\n\n\n\n\n\nloss, accuracy = model5.evaluate(test_ds)\n\nprint(f\"Test Accuracy: {accuracy}\")\n\n37/37 [==============================] - 1s 38ms/step - loss: 0.0794 - accuracy: 0.9695\nTest Accuracy: 0.969475507736206"
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "HW1 : Visualizing Climate Data using Query & Database",
    "section": "",
    "text": "In this blog post, we’ll look through interesting dataset on global temperatures and explore various ways of visualize using Python, SQLite, and Plotly. We will create interactive visualizations that can help us understand worldwide temperature over time.\n\n# To properly show figures in your blog,\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/hw1/index.html#yearly-temperature-variation",
    "href": "posts/hw1/index.html#yearly-temperature-variation",
    "title": "HW1 : Visualizing Climate Data using Query & Database",
    "section": "1) Yearly Temperature Variation",
    "text": "1) Yearly Temperature Variation\nQuestion: How has the temperature variation (difference between the highest and lowest temperatures) changed over the years for a given country?\nTo answer this question, yearly_temperature_variation will calculate the yearly temperature variation for a specified country and range of years.\nThis is what the yearly_temperature_variation query function looks like :\n\nfrom climate_database import yearly_temperature_variation\nimport inspect\nprint(inspect.getsource(yearly_temperature_variation))\n\ndef yearly_temperature_variation(db_file, country, start_year, end_year):\n    \"\"\"\n    Yearly temperature variation (difference between max and min temperatures) \n    for a given country and start year and end year.\n    \n    Returns: Dataframe with two columns: Year and Variation,\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    query = f\"\"\"\n    SELECT T.Year, (MAX(T.Temp) - MIN(T.Temp)) AS Variation\n    FROM temperatures T\n    JOIN stations S ON T.ID = S.ID\n    JOIN countries C ON substr(T.ID, 1, 2) = C.[FIPS 10-4]\n    WHERE C.Name = ? AND T.Year BETWEEN ? AND ?\n    GROUP BY T.Year\n    ORDER BY T.Year;\n    \"\"\"\n    df = pd.read_sql_query(query, conn, params=(country, start_year, end_year))\n    conn.close()\n    return df\n\n\n\nWith the plot_yearly_temperature_variation, we can create a line plot showing the temperature variation of a specific country.\n\ndef plot_yearly_temperature_variation(db_file, country, start_year, end_year):\n    \"\"\"\n    Creates a line plot showing the yearly temperature variation.\n    \"\"\"\n    \n    df = yearly_temperature_variation(db_file, country, start_year, end_year)\n    fig = px.line(df, x='Year', y='Variation', \n                  title=f'Yearly Temperature Variation in {country}')\n    fig.update_xaxes(title_text='Year')\n    fig.update_yaxes(title_text='Temperature Variation (°C)')\n    return fig\n\n\nplot_yearly_temperature_variation(\"climate_data.db\", \"Japan\", 1960, 2000)"
  },
  {
    "objectID": "posts/hw1/index.html#extreme-temperature-on-specific-year-by-month",
    "href": "posts/hw1/index.html#extreme-temperature-on-specific-year-by-month",
    "title": "HW1 : Visualizing Climate Data using Query & Database",
    "section": "2) Extreme Temperature on Specific year by Month",
    "text": "2) Extreme Temperature on Specific year by Month\nQuestion : “What are the hottest and coldest temperatures for each month in a given year?”\nThe second query function is plot_extreme_temperatures(df, year) function which get the extreme temperature, the highest and the lowest, in the world when you put the input year. It will show you facet plot with the lowest and the highest temperatre plots in seperate figure.\n\nfrom climate_database import extreme_temperatures\nimport inspect\nprint(inspect.getsource(extreme_temperatures))\n\ndef extreme_temperatures(db_file, year):\n    \"\"\"\n    Lowest and highest temperatures for each month in specific year.\n    \n    Returns: Dataframe containing the lowest and highest temperatures for each month \n    in the world.\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cmd = f\"\"\"\n    SELECT {year} AS Year, Month,\n           MIN(Temp) AS Min_Temperature,\n           MAX(Temp) AS Max_Temperature\n    FROM temperatures\n    WHERE Year = '{year}'\n    GROUP BY Month\n    ORDER BY Month\n    \"\"\"\n    df = pd.read_sql_query(cmd, conn)\n    # Prepare the DataFrame for faceting by melting it\n    df_melted = df.melt(id_vars=['Month'], value_vars=['Min_Temperature', 'Max_Temperature'],\n                        var_name='Temperature_Type', value_name='Temperature')\n    conn.close()\n    return df_melted\n\n\n\n\nimport plotly.express as px\n\ndef plot_extreme_temperatures(df, year):\n    \"\"\"\n    Creates a faceted line plot showing the lowest and highest temperatures for each month.\n    \n    Parameters:\n    - df (DataFrame): DataFrame containing the lowest and highest temperatures for each month, \n        which we got from the query.\n    - year (int): The year for which extreme temperatures are visualized.\n    \"\"\"\n    # Use the DataFrame directly for plotting with Plotly Express\n    fig = px.line(df, x='Month', y='Temperature', color='Temperature_Type', \n                  facet_col='Temperature_Type', title=f'Extreme Temperatures in {year}')\n    \n    # Update layout for better readability\n    fig.update_layout(xaxis_title='Month', yaxis_title='Temperature (°C)')\n    fig.show()\n\n\n# Plot using the year you want \n# Ex. year 2000\nplot_extreme_temperatures(extreme_temperatures(\"climate_data.db\", 2000), 2000)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BlogPic16B",
    "section": "",
    "text": "HW3 : Message Bank Web Development Using Flask\n\n\n\n\n\n\nweek 4\n\n\nweek 5\n\n\nHW\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW2 : Scrapying TMDB Website & Movie Recommandation\n\n\n\n\n\n\nweek 3\n\n\nweek 4\n\n\nHW\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW1 : Visualizing Climate Data using Query & Database\n\n\n\n\n\n\nweek 2\n\n\nweek 3\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHW0 Creating Post\n\n\n\n\n\n\nweek 1\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nDonggyu Kim\n\n\n\n\n\n\nNo matching items"
  }
]